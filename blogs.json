{"status":"ok","feed":{"url":"https://medium.com/feed/@karteekmenda93","title":"Stories by Karteek Menda on Medium","link":"https://medium.com/@karteekmenda93?source=rss-41e101cb82b5------2","author":"","description":"Stories by Karteek Menda on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*iCYxbuYCWfUIpM8p-C_hMA.jpeg"},"items":[{"title":"Control the heater and filter the data for better performance","pubDate":"2024-01-06 08:35:02","link":"https://medium.com/@karteekmenda93/control-the-heater-and-filter-the-data-for-better-performance-c276b43e1b4b?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/c276b43e1b4b","author":"Karteek Menda","thumbnail":"","description":"\n<p>Hello Aliens</p>\n<p>In the ever-evolving landscape of mechatronics, the integration of mechanical and electronic systems has become instrumental in enhancing the performance and efficiency of various applications. One such imperative project in the realm of mechatronics involves the control of a heater system coupled with advanced data filtering mechanisms. This project is designed to address the critical need for precise temperature control and optimized data processing, catering to diverse industries ranging from manufacturing and healthcare to environmental monitoring.</p>\n<p>The primary focus of this mechatronics project lies in the seamless integration of a heater control system with cutting-edge data filtering techniques. By combining the principles of mechanical engineering, electronics, and data science, this project aims to achieve enhanced performance, energy efficiency, and accuracy in temperature regulation. The controlled heating aspect is essential in applications where maintaining a specific temperature is paramount, such as industrial processes, laboratories, or climate-controlled environments.</p>\n<p>Simultaneously, the incorporation of robust data filtering mechanisms adds a layer of intelligence to the system. The efficient processing and analysis of data generated during the heating process contribute to improved decision-making and overall system optimization. This aspect of the project finds relevance in scenarios where real-time monitoring and data-driven insights are crucial, ensuring that the system operates at peak performance while adapting to dynamic environmental conditions.</p>\n<p>As the field of mechatronics continues to advance, the synergy between mechanical components and intelligent electronic control systems becomes increasingly pivotal.</p>\n<p>So, lets dive deeper starting with the\u00a0Model</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3N58qglb3D1HUpo49y7shg.png\"></figure><h4>Description of the Simulink\u00a0Model</h4>\n<p>We have two chunks of code. The first chunk of code is about the data we need to know. Breaking down each of the components in the <em>first\u00a0chunk</em>.</p>\n<ol>\n<li>\n<strong><em>Arduino Analog Input:</em></strong> Measure the voltage of an analog input pin 0. The block outputs the voltage of the specified pin (0) as a digital value. This voltage is measured as a 10-bit value ranging between 0 and 1023. An output of 0 indicates that the voltage specified at pin 0 equals the ground voltage, and 1023 indicates that the voltage at pin 0 equals the analog reference voltage. The sample time is set to half a\u00a0second</li>\n<li>The output of the analog input is a 10-bit integer. So, I made it <strong><em>single</em></strong> to make it a real\u00a0number</li>\n<li>The real number can be multiplied by a <strong><em>gain</em></strong>. My gain is 5 volts, which corresponds to 1023 bits (2\u00b9\u2070\u00a0-1)</li>\n<li>Then I subtracted an <strong><em>offset</em></strong> (based on the reference from the data sheet, it is a half-volt offset) as the temperature is proportional to the\u00a0voltage</li>\n<li>My <strong><em>gain</em></strong> (element-wise) would be basically a hundred degrees Celsius per\u00a0Volt</li>\n<li>That is how I got the <strong><em>temperature</em></strong> as\u00a0output</li>\n<li>I added a <strong><em>scope</em></strong> block to display the simulation results</li>\n</ol>\n<p><strong><em>Second Chunk</em></strong>: The heater needs to be turned on and off with pulse modulation.</p>\n<p>1. As I setup a duty cycle of 80%, 40%, and off. The <strong><em>step function</em></strong> conveys that you should not do anything for 10 seconds and then turn it on at 80% for the first 500 seconds, 40% for the next 500 seconds, and 0% for the next 500\u00a0seconds.</p>\n<p>2. To turn that into a real number. I used <strong><em>Single</em></strong>. And as I wanted to save my <strong><em>duty cycle</em></strong>. As my duty cycle is an input and temperature is the\u00a0output.</p>\n<p>3. <strong><em>Saturation</em></strong>: The duty cycle has to be between 0 and\u00a0100.</p>\n<p>4. The duty cycle has to be converted to a number between 255 and 0. 255 is the maximum and is divided by 100 duty cycles, which is the <strong><em>gain</em></strong> function.</p>\n<p>5. As I wanted to send it out, it had to be an <strong><em>unsigned\u00a0</em></strong>integer.</p>\n<p>6. <strong><em>Arduino PWM</em></strong>: Generates the square waveform on the output pin number 3 (heater 1). The block accepts values between 0 and 255. The input controls the duty cycle of the square waveform. An input value of 0 produces a 0% duty cycle, and an input value of 255 produces a 100% duty\u00a0cycle.</p>\n<p>I have temperature from the sensor that is glued to the transistor. Once the above Simulink model is run. It is compiled and downloaded to the microprocessor, and the temperature data starts incoming, and that is\u00a0saved.</p>\n<p>Below is the temperature vs time plot for the three duty cycles of 80%, 40% and 0% (500 seconds\u00a0each).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gKs1DSIw-RvqugVC6-j4_A.png\"></figure><h4>Filtering the temperature data</h4>\n<p>I had the temperature data. Time vs temperature data. The same excel file data is read in. The first column of data is time, and the second column is temperature. It reads the entire data from the Excel sheet \u201cProject_Temperature_data.xls\u201d. If we look at the plot, it is going up and down. This might be one or two bits flipping.</p>\n<p>I built a low-pass filter( 3rd\u00a0order)</p>\n<p>Infinite Impulse Response\u00a0filter</p>\n<pre>N = 3; % Order<br>Fpass = 0.05; % Passband Frequency<br>Apass = 1; % Passband Ripple (dB)<br>Fs = 0.5; % Sampling Frequency<br>h = fdesign.lowpass('n,fp,ap', N, Fpass, Apass, Fs);% design a lowpass filter with all these above coefficients<br>Hd = design(h, 'cheby1', ...<br>'SOSScaleNorm', 'Linf');<br>% Hd coefficients of the filter and the gain values<br>% Hd =<br>% FilterStructure: [1x37 char]<br>% Arithmetic: 'double'<br>% sosMatrix: [2x6 double]<br>% ScaleValues: [0.145707496327704;1;1]<br>% OptimizeScaleVal ues: true<br>% PersistentMemory: false</pre>\n<p>Filter Response, which is the bode plot of the filter. I want to make sure the filter is nice and flat up to 0.50 Hz and then drops\u00a0off.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b9sJLG7vju9u5PsbebwsDA.png\"><figcaption>Bode Diagram</figcaption></figure><pre>sosMatrix = Hd.sosMatrix; % sub part of the matrix<br>gain = Hd.ScaleValues; % scale values<br>temp1 = temp; % temperature data<br>offset = temp1(1); % offset id the first value of temperature data 25.26 degree<br>%offset = mean(temp1)<br>temp1 = temp1-offset; % subtract the offset everywhere and that is the data I filtered out<br>y = prod(gain)*sosfilt(sosMatrix, temp1); % y is the output<br>figure;<br>plot(time,temp,time,y+offset,'r--')% add the offset back in to make it better</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1scLinXi7qL1XzozpohRmw.png\"><figcaption>3rd order\u00a0filter</figcaption></figure><p>Zoom in on the image to show the filter has done its job to a greater\u00a0extent.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Zg7zRuuicD637NofU9kENA.png\"></figure><p>All the noise kind of went away. There is no offset and some stuff at the beginning. So, in this project, I collected the temperature data and filtered the data for better performance.</p>\n<p>You can see the video\u00a0below.</p>\n<a href=\"https://medium.com/media/e9ed981a798fb5986813e733640f1885/href\">https://medium.com/media/e9ed981a798fb5986813e733640f1885/href</a><p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c276b43e1b4b\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Hello Aliens</p>\n<p>In the ever-evolving landscape of mechatronics, the integration of mechanical and electronic systems has become instrumental in enhancing the performance and efficiency of various applications. One such imperative project in the realm of mechatronics involves the control of a heater system coupled with advanced data filtering mechanisms. This project is designed to address the critical need for precise temperature control and optimized data processing, catering to diverse industries ranging from manufacturing and healthcare to environmental monitoring.</p>\n<p>The primary focus of this mechatronics project lies in the seamless integration of a heater control system with cutting-edge data filtering techniques. By combining the principles of mechanical engineering, electronics, and data science, this project aims to achieve enhanced performance, energy efficiency, and accuracy in temperature regulation. The controlled heating aspect is essential in applications where maintaining a specific temperature is paramount, such as industrial processes, laboratories, or climate-controlled environments.</p>\n<p>Simultaneously, the incorporation of robust data filtering mechanisms adds a layer of intelligence to the system. The efficient processing and analysis of data generated during the heating process contribute to improved decision-making and overall system optimization. This aspect of the project finds relevance in scenarios where real-time monitoring and data-driven insights are crucial, ensuring that the system operates at peak performance while adapting to dynamic environmental conditions.</p>\n<p>As the field of mechatronics continues to advance, the synergy between mechanical components and intelligent electronic control systems becomes increasingly pivotal.</p>\n<p>So, lets dive deeper starting with the\u00a0Model</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3N58qglb3D1HUpo49y7shg.png\"></figure><h4>Description of the Simulink\u00a0Model</h4>\n<p>We have two chunks of code. The first chunk of code is about the data we need to know. Breaking down each of the components in the <em>first\u00a0chunk</em>.</p>\n<ol>\n<li>\n<strong><em>Arduino Analog Input:</em></strong> Measure the voltage of an analog input pin 0. The block outputs the voltage of the specified pin (0) as a digital value. This voltage is measured as a 10-bit value ranging between 0 and 1023. An output of 0 indicates that the voltage specified at pin 0 equals the ground voltage, and 1023 indicates that the voltage at pin 0 equals the analog reference voltage. The sample time is set to half a\u00a0second</li>\n<li>The output of the analog input is a 10-bit integer. So, I made it <strong><em>single</em></strong> to make it a real\u00a0number</li>\n<li>The real number can be multiplied by a <strong><em>gain</em></strong>. My gain is 5 volts, which corresponds to 1023 bits (2\u00b9\u2070\u00a0-1)</li>\n<li>Then I subtracted an <strong><em>offset</em></strong> (based on the reference from the data sheet, it is a half-volt offset) as the temperature is proportional to the\u00a0voltage</li>\n<li>My <strong><em>gain</em></strong> (element-wise) would be basically a hundred degrees Celsius per\u00a0Volt</li>\n<li>That is how I got the <strong><em>temperature</em></strong> as\u00a0output</li>\n<li>I added a <strong><em>scope</em></strong> block to display the simulation results</li>\n</ol>\n<p><strong><em>Second Chunk</em></strong>: The heater needs to be turned on and off with pulse modulation.</p>\n<p>1. As I setup a duty cycle of 80%, 40%, and off. The <strong><em>step function</em></strong> conveys that you should not do anything for 10 seconds and then turn it on at 80% for the first 500 seconds, 40% for the next 500 seconds, and 0% for the next 500\u00a0seconds.</p>\n<p>2. To turn that into a real number. I used <strong><em>Single</em></strong>. And as I wanted to save my <strong><em>duty cycle</em></strong>. As my duty cycle is an input and temperature is the\u00a0output.</p>\n<p>3. <strong><em>Saturation</em></strong>: The duty cycle has to be between 0 and\u00a0100.</p>\n<p>4. The duty cycle has to be converted to a number between 255 and 0. 255 is the maximum and is divided by 100 duty cycles, which is the <strong><em>gain</em></strong> function.</p>\n<p>5. As I wanted to send it out, it had to be an <strong><em>unsigned\u00a0</em></strong>integer.</p>\n<p>6. <strong><em>Arduino PWM</em></strong>: Generates the square waveform on the output pin number 3 (heater 1). The block accepts values between 0 and 255. The input controls the duty cycle of the square waveform. An input value of 0 produces a 0% duty cycle, and an input value of 255 produces a 100% duty\u00a0cycle.</p>\n<p>I have temperature from the sensor that is glued to the transistor. Once the above Simulink model is run. It is compiled and downloaded to the microprocessor, and the temperature data starts incoming, and that is\u00a0saved.</p>\n<p>Below is the temperature vs time plot for the three duty cycles of 80%, 40% and 0% (500 seconds\u00a0each).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gKs1DSIw-RvqugVC6-j4_A.png\"></figure><h4>Filtering the temperature data</h4>\n<p>I had the temperature data. Time vs temperature data. The same excel file data is read in. The first column of data is time, and the second column is temperature. It reads the entire data from the Excel sheet \u201cProject_Temperature_data.xls\u201d. If we look at the plot, it is going up and down. This might be one or two bits flipping.</p>\n<p>I built a low-pass filter( 3rd\u00a0order)</p>\n<p>Infinite Impulse Response\u00a0filter</p>\n<pre>N = 3; % Order<br>Fpass = 0.05; % Passband Frequency<br>Apass = 1; % Passband Ripple (dB)<br>Fs = 0.5; % Sampling Frequency<br>h = fdesign.lowpass('n,fp,ap', N, Fpass, Apass, Fs);% design a lowpass filter with all these above coefficients<br>Hd = design(h, 'cheby1', ...<br>'SOSScaleNorm', 'Linf');<br>% Hd coefficients of the filter and the gain values<br>% Hd =<br>% FilterStructure: [1x37 char]<br>% Arithmetic: 'double'<br>% sosMatrix: [2x6 double]<br>% ScaleValues: [0.145707496327704;1;1]<br>% OptimizeScaleVal ues: true<br>% PersistentMemory: false</pre>\n<p>Filter Response, which is the bode plot of the filter. I want to make sure the filter is nice and flat up to 0.50 Hz and then drops\u00a0off.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b9sJLG7vju9u5PsbebwsDA.png\"><figcaption>Bode Diagram</figcaption></figure><pre>sosMatrix = Hd.sosMatrix; % sub part of the matrix<br>gain = Hd.ScaleValues; % scale values<br>temp1 = temp; % temperature data<br>offset = temp1(1); % offset id the first value of temperature data 25.26 degree<br>%offset = mean(temp1)<br>temp1 = temp1-offset; % subtract the offset everywhere and that is the data I filtered out<br>y = prod(gain)*sosfilt(sosMatrix, temp1); % y is the output<br>figure;<br>plot(time,temp,time,y+offset,'r--')% add the offset back in to make it better</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1scLinXi7qL1XzozpohRmw.png\"><figcaption>3rd order\u00a0filter</figcaption></figure><p>Zoom in on the image to show the filter has done its job to a greater\u00a0extent.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Zg7zRuuicD637NofU9kENA.png\"></figure><p>All the noise kind of went away. There is no offset and some stuff at the beginning. So, in this project, I collected the temperature data and filtered the data for better performance.</p>\n<p>You can see the video\u00a0below.</p>\n<a href=\"https://medium.com/media/e9ed981a798fb5986813e733640f1885/href\">https://medium.com/media/e9ed981a798fb5986813e733640f1885/href</a><p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c276b43e1b4b\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["temperature-controller","filters","data-filtering","mechatronics"]},{"title":"Tic-tac-toe with MyCobot\u200a\u2014\u200aPart 2","pubDate":"2023-12-26 02:59:11","link":"https://medium.com/@karteekmenda93/tic-tac-toe-with-mycobot-part-2-aba39fcf7811?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/aba39fcf7811","author":"Karteek Menda","thumbnail":"","description":"\n<h3>Tic-tac-toe with MyCobot\u200a\u2014\u200aPart\u00a02</h3>\n<p>Hello Aliens</p>\n<p>Continuing from the <a href=\"https://medium.com/@karteekmenda93/tic-tac-toe-with-mycobot-part-1-71e9ab2f1432\">previous blog</a> post, the upcoming content will provide an overview of the Minimax Algorithm, delve into the task setup, and illustrate the seamless integration of these components. The blog will explore how each element contributes to the successful execution of the task at hand, offering a comprehensive understanding of the Minimax Algorithm and its practical application within the specified context.</p>\n<h4>Minimax Algorithm</h4>\n<p>The minimax algorithm is a decision-making algorithm commonly used in two-player turn-based games such as chess, checkers, or tic-tac-toe. The primary goal of the minimax algorithm is to determine the optimal move for a player, considering all possible moves and their potential outcomes.</p>\n<p>Here\u2019s a simplified explanation of how the Minimax algorithm works:</p>\n<ol>\n<li>\n<em>Evaluation Function:</em> Assign a value to each possible game state. This value represents how favorable the state is for the current player. For example, in a chess game, a positive value might indicate an advantageous position, while a negative value might suggest a disadvantage.</li>\n<li>\n<em>Recursive Tree Search:</em> Generate a tree of possible moves, where each level alternates between the maximizing player (trying to maximize the evaluation function) and the minimizing player (trying to minimize the evaluation function). Recursively evaluate each node in the tree until reaching a specified depth or a terminal game\u00a0state.</li>\n<li>\n<em>Backtracking: </em>Propagate the values from the terminal nodes back up the tree, adjusting the values based on whether it\u2019s the turn of the maximizing or minimizing player. At each level, the maximizing player chooses the move that leads to the maximum value, while the minimizing player chooses the move that leads to the minimum\u00a0value.</li>\n<li>\n<em>Optimal Move:</em> The root of the tree represents the initial game state, and the algorithm selects the move that leads to the child node with the highest value, assuming both players play optimally.</li>\n</ol>\n<p>This algorithm ensures that the AI player makes decisions with the assumption that the opponent will also make optimal moves. However, it has limitations in terms of the computational resources required, especially as the game tree grows exponentially. To mitigate this, optimizations like alpha-beta pruning are often applied to reduce the number of branches that need to be explored.</p>\n<p>A back and forth between both players, along with the player deciding whose \u201cturn it is,\u201d is crucial to the Minimax algorithm to be able to determine which player has the best result. The other player then selects which of its possible actions has the lowest possible score, calculating the scores for each of the possible actions. The player taking turns subsequently attempts to maximize its score, and so on along the move tree into an end state, deciding the scores for the moves of the opposing\u00a0players.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/567/1*ggFwdYEyb2fh123tNR2RLw.png\"></figure><p>Walking through the algorithm execution.</p>\n<ol>\n<li>In state 1, it is now the X\u2019s turn. X runs minimax on states 2, 3, and 4 upon generating them.</li>\n<li>Since the match is at its final state, the second state gives the score of +10 to state 1\u2019s score\u00a0list.</li>\n<li>Without being in the end states, states 3 and 4 generate states 5 and 6 and use minimax on them, whereas states 4 create states 7 and 8 and use minimax on\u00a0them.</li>\n<li>State 5 places a score of -10 on the rating list of state 3, while state 7 performs the same, giving a score of -10 on the score list of state\u00a04.</li>\n<li>States 6 and 8 add a rating of +10 to the move lists of states 3 and 4 as they generate the only feasible moves, which are end\u00a0states.</li>\n<li>As it is O\u2019s turn in states 3 and 4, O is going to attempt to find the smallest score; given the option, the two states 3 and 4 will result in a score of\u00a0-10.</li>\n<li>State 1 will choose the winner\u2019s move with a score of +10 in a bid to best utilise the score, while state 2 will fill up the score list for states 2, 3, and 4 with +10, -10, and -10, respectively.</li>\n</ol>\n<p>To summarize, the minimax algorithm works on game tree representations, backward recursion (minimax), and decision-making. While playing a game like tic-tac-toe, the algorithm evaluates all of the potential moves, gives a numerical value for every game state, and chooses the move that gives the participant the best outcome at the time after taking the opponent\u2019s best response into consideration.</p>\n<h4>Setup</h4>\n<p>Installing <a href=\"https://pypi.org/project/pymycobot/\">Pymycobot</a> and the necessary Python software is the first step. You must install myCobot for Linux after installing Python. Next, you must download and burn the transponder while the base is linked to the display. Next, open myStudio, download <strong>Atom</strong>, and <strong>flash</strong> it. To stop the robot from hitting the camera module, simply install the required software and then set all motor settings to <strong>0 </strong>degrees. After that, use the <a href=\"https://github.com/KarteekMenda93/pymycobot/blob/main/Calibration.py\">calibration code</a> to calibrate. In addition to it, install the necessary OpenCV packages as well. Make sure the environment is properly set for error-free execution. Remember, this task is nothing but establishing a nice integration of Robotics, Artificial intelligence, and Computer\u00a0Vision.</p>\n<a href=\"https://medium.com/media/f8582cdf6373ae2be3ef4faa6d4e0d75/href\">https://medium.com/media/f8582cdf6373ae2be3ef4faa6d4e0d75/href</a><p>The logic encoded in the script consists of some methods related to the Minimax algorithm that are integrated.</p>\n<p><strong>a. Creation of tic-tac-toe board</strong></p>\n<ol>\n<li>\n<em>board_initialization():</em> as I set the columns and rows to 3 each. I want to create a board in a 3x3 matrix shape that represents each cell in the\u00a0game.</li>\n<li>\n<em>draw_board():</em> takes an image as input and draws a tic-tac-toe board on it. The board is divided into a grid with horizontal and vertical lines. The black colour lines are used with a line thickness of 3\u00a0pixels.</li>\n<li>\n<em>print_board():</em> displays the tic-tac-toe board in the\u00a0console.</li>\n</ol>\n<p><strong>b. Centers for the blocks in the\u00a0game</strong></p>\n<ol><li>\n<em>block_centers()</em>: got these coordinates by collecting all the 9 block coordinates and their\u00a0centers.</li></ol>\n<p><strong>c. Getting the pickup places and the other relevant coordinates</strong></p>\n<ol><li>\n<em>pickup_place_cords():</em> Giving the coordinates to the myCobot to pickup the block for its (computer) move. And placing the blocks at the mentioned centers, which I declared in the <em>block_centers()</em>.</li></ol>\n<p><strong>d. Minimax algorithm</strong></p>\n<ol><li>\n<em>minimax(): </em>This function is a recursive implementation of the minimax algorithm for a tic-tac-toe game. The function evaluates the game state and returns a score based on whether the \u2018O\u2019 player or the \u2018X\u2019 player is maximizing their\u00a0score.</li></ol>\n<p>Base Cases:</p>\n<p>If the \u2018O\u2019 player has won, the function returns\u00a01.</p>\n<p>If the \u2018X\u2019 player has won, the function returns\u00a0-1.</p>\n<p>If the board is full and there\u2019s no winner, the function returns\u00a00.</p>\n<p>Recursive Cases:</p>\n<p>If it\u2019s the turn for the maximizing player (currently \u2018O\u2019), the function iterates through available <em>pickup_place_cords</em>, simulates each <em>pickup_place_cords</em>, and calls itself recursively with the updated board and the next player\u2019s turn (\u2018X\u2019). It then updates max_eval with the maximum value obtained from these recursive calls.</p>\n<p>If it\u2019s the turn for the minimizing player (currently \u2018X\u2019), the function does the same but minimizes the values by updating min_eval with the minimum value obtained from recursive calls.</p>\n<p><em>Undoing pickup_place_cords:</em> After each pickup_place_cords simulation, the board is reverted to its original state by resetting the cell to an empty\u00a0space.</p>\n<p>2. <em>find_best_pickup_place_cords()\u00a0:</em> AI algorithm for playing Tic-Tac-Toe. It uses the minimax algorithm to evaluate and choose the best pickup_place_cords for the \u2018O\u2019\u00a0player.</p>\n<p>3. <em>cross_check_winner()\u00a0:</em> Checks which player has won the game. In order to do that, need to check the rows and columns and also the diagonals.</p>\n<p>Credits to <a href=\"https://shop.elephantrobotics.com/?utm_term=elephant%20robotics&amp;utm_campaign=%E3%80%90%E4%B8%8D%E8%83%BD%E8%B6%85%E8%BF%87430%E3%80%91Elephantrobotics%E5%93%81%E7%89%8C%E5%90%8D&amp;utm_source=adwords&amp;utm_medium=ppc&amp;dm_acc=3657328933&amp;dm_cam=15616166851&amp;dm_grp=131630769775&amp;dm_ad=570368651482&amp;dm_src=g&amp;dm_tgt=kwd-443848233998&amp;dm_kw=elephant%20robotics&amp;dm_mt=b&amp;dm_net=adwords&amp;dm_ver=3&amp;gad_source=1&amp;gclid=Cj0KCQiA7aSsBhCiARIsALFvovxg11KhxkYvYmgA-WIbEe9glDd0prWhMkR01-3s8tWue3WTSpCjMAAaAsOJEALw_wcB\">Elephant Robotics</a> for the color recognition code.</p>\n<p>The Aruco\u2019s (markers) were detected upon which the grids (tic tac toe game board) get activated and displayed on the screen. Place the green blocks in the bin from which the cobot picks up. Apart from it, I need to have the yellow cubes. So, as a part of the first move, I need to put the yellow block in one of the 9 grids, and the camera module identifies the block and based on the minimax algorithm\u2019s next decision of making a move, the cobot pickups the green block from the bin and puts it in the grid as guided by the minimax algorithm. Now, it is my turn to pace yellow block in the grid which ever I want. And then the cobot takes it move and this process continues on until an end result is achieved. You can see the video\u00a0below</p>\n<a href=\"https://medium.com/media/5756d547a273e41d69743f776ec2d7f2/href\">https://medium.com/media/5756d547a273e41d69743f776ec2d7f2/href</a><p>In my video, the game was won by the computer as it followed the decisions from the Minimax algorithm. The robot advances to the green block and picks up the green block and put it in the respective coordinates as directed by the logic by the Minimax algorithm. In my turn I need to keep the yellow block in the grid which ever I want to place. The process continues on. This demonstrates the cobot\u2019s capabilities by using the color recognition. The cobot\u2019s capabilities, decision making algorithm (AI Move), and the computer vision all combinedly comes together to get this task\u00a0done.</p>\n<h4>References</h4>\n<p>[1] <a href=\"https://www.hackster.io/Elephant-Robotics-Official/newly-upgraded-and-user-friendly-ai-kit-2023-%20a23669\">https://www.hackster.io/Elephant-Robotics-Official/newly-upgraded-and-user-friendly-ai-kit-2023- a23669</a></p>\n<p>[2] <a href=\"https://docs.elephantrobotics.com/docs/gitbook-en/2-serialproduct/2.10-AIkit2023en_320/3-%20color_recognition.html\">https://docs.elephantrobotics.com/docs/gitbook-en/2-serialproduct/2.10-AIkit2023en_320/3- color_recognition.html</a></p>\n<p>[3] <a href=\"https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-1-introduction/\">https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-1-introduction/</a></p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aba39fcf7811\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Tic-tac-toe with MyCobot\u200a\u2014\u200aPart\u00a02</h3>\n<p>Hello Aliens</p>\n<p>Continuing from the <a href=\"https://medium.com/@karteekmenda93/tic-tac-toe-with-mycobot-part-1-71e9ab2f1432\">previous blog</a> post, the upcoming content will provide an overview of the Minimax Algorithm, delve into the task setup, and illustrate the seamless integration of these components. The blog will explore how each element contributes to the successful execution of the task at hand, offering a comprehensive understanding of the Minimax Algorithm and its practical application within the specified context.</p>\n<h4>Minimax Algorithm</h4>\n<p>The minimax algorithm is a decision-making algorithm commonly used in two-player turn-based games such as chess, checkers, or tic-tac-toe. The primary goal of the minimax algorithm is to determine the optimal move for a player, considering all possible moves and their potential outcomes.</p>\n<p>Here\u2019s a simplified explanation of how the Minimax algorithm works:</p>\n<ol>\n<li>\n<em>Evaluation Function:</em> Assign a value to each possible game state. This value represents how favorable the state is for the current player. For example, in a chess game, a positive value might indicate an advantageous position, while a negative value might suggest a disadvantage.</li>\n<li>\n<em>Recursive Tree Search:</em> Generate a tree of possible moves, where each level alternates between the maximizing player (trying to maximize the evaluation function) and the minimizing player (trying to minimize the evaluation function). Recursively evaluate each node in the tree until reaching a specified depth or a terminal game\u00a0state.</li>\n<li>\n<em>Backtracking: </em>Propagate the values from the terminal nodes back up the tree, adjusting the values based on whether it\u2019s the turn of the maximizing or minimizing player. At each level, the maximizing player chooses the move that leads to the maximum value, while the minimizing player chooses the move that leads to the minimum\u00a0value.</li>\n<li>\n<em>Optimal Move:</em> The root of the tree represents the initial game state, and the algorithm selects the move that leads to the child node with the highest value, assuming both players play optimally.</li>\n</ol>\n<p>This algorithm ensures that the AI player makes decisions with the assumption that the opponent will also make optimal moves. However, it has limitations in terms of the computational resources required, especially as the game tree grows exponentially. To mitigate this, optimizations like alpha-beta pruning are often applied to reduce the number of branches that need to be explored.</p>\n<p>A back and forth between both players, along with the player deciding whose \u201cturn it is,\u201d is crucial to the Minimax algorithm to be able to determine which player has the best result. The other player then selects which of its possible actions has the lowest possible score, calculating the scores for each of the possible actions. The player taking turns subsequently attempts to maximize its score, and so on along the move tree into an end state, deciding the scores for the moves of the opposing\u00a0players.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/567/1*ggFwdYEyb2fh123tNR2RLw.png\"></figure><p>Walking through the algorithm execution.</p>\n<ol>\n<li>In state 1, it is now the X\u2019s turn. X runs minimax on states 2, 3, and 4 upon generating them.</li>\n<li>Since the match is at its final state, the second state gives the score of +10 to state 1\u2019s score\u00a0list.</li>\n<li>Without being in the end states, states 3 and 4 generate states 5 and 6 and use minimax on them, whereas states 4 create states 7 and 8 and use minimax on\u00a0them.</li>\n<li>State 5 places a score of -10 on the rating list of state 3, while state 7 performs the same, giving a score of -10 on the score list of state\u00a04.</li>\n<li>States 6 and 8 add a rating of +10 to the move lists of states 3 and 4 as they generate the only feasible moves, which are end\u00a0states.</li>\n<li>As it is O\u2019s turn in states 3 and 4, O is going to attempt to find the smallest score; given the option, the two states 3 and 4 will result in a score of\u00a0-10.</li>\n<li>State 1 will choose the winner\u2019s move with a score of +10 in a bid to best utilise the score, while state 2 will fill up the score list for states 2, 3, and 4 with +10, -10, and -10, respectively.</li>\n</ol>\n<p>To summarize, the minimax algorithm works on game tree representations, backward recursion (minimax), and decision-making. While playing a game like tic-tac-toe, the algorithm evaluates all of the potential moves, gives a numerical value for every game state, and chooses the move that gives the participant the best outcome at the time after taking the opponent\u2019s best response into consideration.</p>\n<h4>Setup</h4>\n<p>Installing <a href=\"https://pypi.org/project/pymycobot/\">Pymycobot</a> and the necessary Python software is the first step. You must install myCobot for Linux after installing Python. Next, you must download and burn the transponder while the base is linked to the display. Next, open myStudio, download <strong>Atom</strong>, and <strong>flash</strong> it. To stop the robot from hitting the camera module, simply install the required software and then set all motor settings to <strong>0 </strong>degrees. After that, use the <a href=\"https://github.com/KarteekMenda93/pymycobot/blob/main/Calibration.py\">calibration code</a> to calibrate. In addition to it, install the necessary OpenCV packages as well. Make sure the environment is properly set for error-free execution. Remember, this task is nothing but establishing a nice integration of Robotics, Artificial intelligence, and Computer\u00a0Vision.</p>\n<a href=\"https://medium.com/media/f8582cdf6373ae2be3ef4faa6d4e0d75/href\">https://medium.com/media/f8582cdf6373ae2be3ef4faa6d4e0d75/href</a><p>The logic encoded in the script consists of some methods related to the Minimax algorithm that are integrated.</p>\n<p><strong>a. Creation of tic-tac-toe board</strong></p>\n<ol>\n<li>\n<em>board_initialization():</em> as I set the columns and rows to 3 each. I want to create a board in a 3x3 matrix shape that represents each cell in the\u00a0game.</li>\n<li>\n<em>draw_board():</em> takes an image as input and draws a tic-tac-toe board on it. The board is divided into a grid with horizontal and vertical lines. The black colour lines are used with a line thickness of 3\u00a0pixels.</li>\n<li>\n<em>print_board():</em> displays the tic-tac-toe board in the\u00a0console.</li>\n</ol>\n<p><strong>b. Centers for the blocks in the\u00a0game</strong></p>\n<ol><li>\n<em>block_centers()</em>: got these coordinates by collecting all the 9 block coordinates and their\u00a0centers.</li></ol>\n<p><strong>c. Getting the pickup places and the other relevant coordinates</strong></p>\n<ol><li>\n<em>pickup_place_cords():</em> Giving the coordinates to the myCobot to pickup the block for its (computer) move. And placing the blocks at the mentioned centers, which I declared in the <em>block_centers()</em>.</li></ol>\n<p><strong>d. Minimax algorithm</strong></p>\n<ol><li>\n<em>minimax(): </em>This function is a recursive implementation of the minimax algorithm for a tic-tac-toe game. The function evaluates the game state and returns a score based on whether the \u2018O\u2019 player or the \u2018X\u2019 player is maximizing their\u00a0score.</li></ol>\n<p>Base Cases:</p>\n<p>If the \u2018O\u2019 player has won, the function returns\u00a01.</p>\n<p>If the \u2018X\u2019 player has won, the function returns\u00a0-1.</p>\n<p>If the board is full and there\u2019s no winner, the function returns\u00a00.</p>\n<p>Recursive Cases:</p>\n<p>If it\u2019s the turn for the maximizing player (currently \u2018O\u2019), the function iterates through available <em>pickup_place_cords</em>, simulates each <em>pickup_place_cords</em>, and calls itself recursively with the updated board and the next player\u2019s turn (\u2018X\u2019). It then updates max_eval with the maximum value obtained from these recursive calls.</p>\n<p>If it\u2019s the turn for the minimizing player (currently \u2018X\u2019), the function does the same but minimizes the values by updating min_eval with the minimum value obtained from recursive calls.</p>\n<p><em>Undoing pickup_place_cords:</em> After each pickup_place_cords simulation, the board is reverted to its original state by resetting the cell to an empty\u00a0space.</p>\n<p>2. <em>find_best_pickup_place_cords()\u00a0:</em> AI algorithm for playing Tic-Tac-Toe. It uses the minimax algorithm to evaluate and choose the best pickup_place_cords for the \u2018O\u2019\u00a0player.</p>\n<p>3. <em>cross_check_winner()\u00a0:</em> Checks which player has won the game. In order to do that, need to check the rows and columns and also the diagonals.</p>\n<p>Credits to <a href=\"https://shop.elephantrobotics.com/?utm_term=elephant%20robotics&amp;utm_campaign=%E3%80%90%E4%B8%8D%E8%83%BD%E8%B6%85%E8%BF%87430%E3%80%91Elephantrobotics%E5%93%81%E7%89%8C%E5%90%8D&amp;utm_source=adwords&amp;utm_medium=ppc&amp;dm_acc=3657328933&amp;dm_cam=15616166851&amp;dm_grp=131630769775&amp;dm_ad=570368651482&amp;dm_src=g&amp;dm_tgt=kwd-443848233998&amp;dm_kw=elephant%20robotics&amp;dm_mt=b&amp;dm_net=adwords&amp;dm_ver=3&amp;gad_source=1&amp;gclid=Cj0KCQiA7aSsBhCiARIsALFvovxg11KhxkYvYmgA-WIbEe9glDd0prWhMkR01-3s8tWue3WTSpCjMAAaAsOJEALw_wcB\">Elephant Robotics</a> for the color recognition code.</p>\n<p>The Aruco\u2019s (markers) were detected upon which the grids (tic tac toe game board) get activated and displayed on the screen. Place the green blocks in the bin from which the cobot picks up. Apart from it, I need to have the yellow cubes. So, as a part of the first move, I need to put the yellow block in one of the 9 grids, and the camera module identifies the block and based on the minimax algorithm\u2019s next decision of making a move, the cobot pickups the green block from the bin and puts it in the grid as guided by the minimax algorithm. Now, it is my turn to pace yellow block in the grid which ever I want. And then the cobot takes it move and this process continues on until an end result is achieved. You can see the video\u00a0below</p>\n<a href=\"https://medium.com/media/5756d547a273e41d69743f776ec2d7f2/href\">https://medium.com/media/5756d547a273e41d69743f776ec2d7f2/href</a><p>In my video, the game was won by the computer as it followed the decisions from the Minimax algorithm. The robot advances to the green block and picks up the green block and put it in the respective coordinates as directed by the logic by the Minimax algorithm. In my turn I need to keep the yellow block in the grid which ever I want to place. The process continues on. This demonstrates the cobot\u2019s capabilities by using the color recognition. The cobot\u2019s capabilities, decision making algorithm (AI Move), and the computer vision all combinedly comes together to get this task\u00a0done.</p>\n<h4>References</h4>\n<p>[1] <a href=\"https://www.hackster.io/Elephant-Robotics-Official/newly-upgraded-and-user-friendly-ai-kit-2023-%20a23669\">https://www.hackster.io/Elephant-Robotics-Official/newly-upgraded-and-user-friendly-ai-kit-2023- a23669</a></p>\n<p>[2] <a href=\"https://docs.elephantrobotics.com/docs/gitbook-en/2-serialproduct/2.10-AIkit2023en_320/3-%20color_recognition.html\">https://docs.elephantrobotics.com/docs/gitbook-en/2-serialproduct/2.10-AIkit2023en_320/3- color_recognition.html</a></p>\n<p>[3] <a href=\"https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-1-introduction/\">https://www.geeksforgeeks.org/minimax-algorithm-in-game-theory-set-1-introduction/</a></p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=aba39fcf7811\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["robotics","yolov5","tic-tac-toe","minimax-algorithm"]},{"title":"Tic-tac-toe with MyCobot\u200a\u2014\u200aPart 1","pubDate":"2023-12-25 18:38:20","link":"https://medium.com/@karteekmenda93/tic-tac-toe-with-mycobot-part-1-71e9ab2f1432?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/71e9ab2f1432","author":"Karteek Menda","thumbnail":"","description":"\n<h3>Tic-tac-toe with MyCobot\u200a\u2014\u200aPart\u00a01</h3>\n<p>Hello Aliens</p>\n<p>In this series of blog posts, we\u2019ll embark on a journey to achieve the seamless and efficient completion of the tic-tac-toe game by synergistically harnessing the capabilities of MyCobot and strategically implementing the Minimax algorithm.</p>\n<p>Our first step will involve a thorough exploration of the game mechanics of tic-tac-toe. Understanding the foundational rules of the game is crucial for building a solid foundation before incorporating MyCobot and the Minimax algorithm.</p>\n<p>Following our grasp of the game mechanics, we will then delve into an in-depth examination of the versatile capabilities of MyCobot. By understanding the range of functionalities that MyCobot brings to the table, we can identify how to leverage its capabilities to enhance the overall gaming experience.</p>\n<p>Finally, we will unravel the complexities of integrating the Minimax algorithm into the mix. This strategic algorithm will be broken down into its essential elements, providing readers with a comprehensive understanding of how it contributes to the efficient completion of the task at\u00a0hand.</p>\n<p>Join me as I navigate through each component, step by step, to create a holistic and insightful guide on achieving an optimal and automated tic-tac-toe experience with MyCobot and the Minimax algorithm.</p>\n<h4>Tic-Tac-Toe</h4>\n<p>It\u2019s a classic game where two players take turns marking spaces in a 3x3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game. If the grid is filled and no player has three in a row, the game is a\u00a0draw.</p>\n<p>Every player marks a square in turn, usually with a single letter \u201cX\u201d for one player and the letter \"O\" for the other. The goal is to earn three of those points in a row\u200a\u2014\u200avertically, in the horizontal direction, or diagonally\u200a\u2014\u200abefore your rival does\u00a0so</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/225/1*DJz29N7suwDLodoL639Z7g.jpeg\"></figure><h4>MyCobot</h4>\n<p>The myCobot 280 series, created by Elephant Robot, represents a line of 6-degree-of-freedom (6-DOF) collaborative robotic arms designed primarily for research, education, science and technology applications, and commercial exhibitions. Tens of thousands of users worldwide have embraced the convenience and efficiency offered by myCobot arms, utilizing them for learning and implementing robotics across various fields. It has a working radius of 280mm and can handle a payload of\u00a0250g.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/282/0*3hVTrBjtAIi1NW9-.png\"><figcaption>Credits: Elephant\u00a0Robotics</figcaption></figure><ol>\n<li>\n<em>Collaborative Robot (Cobot) Design:</em> Designed as a collaborative robot, myCobot is intended to work alongside humans, fostering a safe and efficient environment for research, education, and other applications.</li>\n<li>\n<em>Research and Education Focus:</em> Tailored specifically for research and educational purposes, myCobot serves as an ideal platform for learning and experimentation in robotics.</li>\n<li>\n<em>Science and Technology Applications: </em>With its advanced technology, myCobot finds utility in a wide range of scientific and technological applications, contributing to various industries.</li>\n<li>\n<em>Commercial Exhibitions:</em> The robot\u2019s capabilities extend to commercial exhibitions, where it can showcase its functionalities and applications to a broader audience.</li>\n</ol>\n<p>Forward Kinematics of\u00a0myCobot</p>\n<a href=\"https://medium.com/media/ec3900d58402741941101b5b9245510b/href\">https://medium.com/media/ec3900d58402741941101b5b9245510b/href</a><p>In 2023, the introduction of the AI Kit (Artificial Intelligence Kit) marks a significant advancement in automation technology. This kit seamlessly integrates visual, positioning, grabbing, and automatic sorting modules to create a comprehensive robotic arm application. Designed to emulate an industrial setting, the AI Kit 2023 revolutionizes manual labor by implementing intelligent sorting and initiating preliminary industrial automation processes.</p>\n<p>At the heart of this simulated industrial scenario are five cutting-edge visual algorithms and motion control algorithms for the robotic arm. These algorithms work in tandem, enabling rapid object recognition and precise sorting capabilities. The AI Kit 2023 represents a leap forward in the pursuit of efficient and smart automation within industrial environments.</p>\n<p>The 5 vision algorithms are:</p>\n<p>\u25cf Shape recognition</p>\n<p>\u25cf Feature point recognition</p>\n<p>\u25cf <strong>ArUco</strong> <strong>code </strong>recognition</p>\n<p>\u25cf Color recognition</p>\n<p>\u25cf <strong>YOLOv5</strong> recognition</p>\n<p>The initial four algorithms of the AI Kit 2023 focus on image processing and machine vision, leveraging OpenCV algorithms. These algorithms encompass color space recognition, feature point recognition, ArUco code recognition, and shape recognition. Each of these components contributes to the kit\u2019s robust visual capabilities, enabling rapid and precise identification of objects within the industrial environment.</p>\n<p>Additionally, the AI Kit 2023 incorporates the YOLOv5 (You Only Look Once version 5) algorithm, recognized as a leading object detection method. YOLOv5 employs Convolutional Neural Networks (CNNs) to predict objects in images swiftly, ensuring rapid detection without compromising on accuracy. As the latest iteration of the YOLO series, version 5 further enhances the AI Kit\u2019s object recognition capabilities, making it a cutting-edge solution for efficient and reliable industrial automation.</p>\n<p>Out of all the above vision algorithms, I tried two (color recognition and YOLOv5) to complete this task. Have a look at the videos for each of the individual.</p>\n<p><strong>Color recognition:</strong></p>\n<p>In this scenario, the AI Kit 2023 operates in an eye-to-hand mode, utilizing a camera and harnessing the power of Python and OpenCV. The system employs OpenCV for color positioning, where it identifies color blocks based on predefined criteria. Once identified, the system frames these color blocks and calculates their relative positions using relevant points within the spatial coordinates of the robotic\u00a0arm.</p>\n<p>The next step involves establishing a set of coordinated actions for the robotic arm, tailored to the specific color and spatial coordinates of the identified objects. The robotic arm then executes these actions, placing the objects in designated areas based on their identified colors. This approach ensures a seamless integration of visual recognition, spatial coordination, and precise manipulation by the robotic arm, facilitating efficient sorting and automation in the industrial setting.</p>\n<a href=\"https://medium.com/media/80894c3a29d5df7a685a4383159d381a/href\">https://medium.com/media/80894c3a29d5df7a685a4383159d381a/href</a><p><strong>YOLO Image Recognition:</strong></p>\n<p>In this application, the AI Kit 2023 employs an eye-to-hand mode, utilizing a camera for image capture. The system utilizes OpenCV to load YOLOv5 model data, facilitating the recognition of image blocks within the captured images. Once identified, the system determines the position of these image blocks within the recognition area, utilizing relevant points for spatial coordinate calculations relative to the robotic\u00a0arm.</p>\n<p>Following recognition and spatial analysis, a predefined set of actions is established for the robotic arm. These actions are tailored to the specific characteristics of the recognized objects. The robotic arm then executes these actions, placing the identified objects into designated areas based on their unique features. This integrated approach seamlessly combines image recognition, spatial coordination, and precise manipulation, showcasing the AI Kit\u2019s ability to automate object recognition and sorting tasks in an industrial context.</p>\n<a href=\"https://medium.com/media/5693ffe42099595884b190a83ed644cd/href\">https://medium.com/media/5693ffe42099595884b190a83ed644cd/href</a><p>Finally, opted to go with the color recognition algorithm, and the setup for this task will be explained in <a href=\"https://medium.com/@karteekmenda93/tic-tac-toe-with-mycobot-part-2-aba39fcf7811\">Part\u00a02</a>.</p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"https://www.linkedin.com/in/karteek-menda/\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=71e9ab2f1432\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Tic-tac-toe with MyCobot\u200a\u2014\u200aPart\u00a01</h3>\n<p>Hello Aliens</p>\n<p>In this series of blog posts, we\u2019ll embark on a journey to achieve the seamless and efficient completion of the tic-tac-toe game by synergistically harnessing the capabilities of MyCobot and strategically implementing the Minimax algorithm.</p>\n<p>Our first step will involve a thorough exploration of the game mechanics of tic-tac-toe. Understanding the foundational rules of the game is crucial for building a solid foundation before incorporating MyCobot and the Minimax algorithm.</p>\n<p>Following our grasp of the game mechanics, we will then delve into an in-depth examination of the versatile capabilities of MyCobot. By understanding the range of functionalities that MyCobot brings to the table, we can identify how to leverage its capabilities to enhance the overall gaming experience.</p>\n<p>Finally, we will unravel the complexities of integrating the Minimax algorithm into the mix. This strategic algorithm will be broken down into its essential elements, providing readers with a comprehensive understanding of how it contributes to the efficient completion of the task at\u00a0hand.</p>\n<p>Join me as I navigate through each component, step by step, to create a holistic and insightful guide on achieving an optimal and automated tic-tac-toe experience with MyCobot and the Minimax algorithm.</p>\n<h4>Tic-Tac-Toe</h4>\n<p>It\u2019s a classic game where two players take turns marking spaces in a 3x3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game. If the grid is filled and no player has three in a row, the game is a\u00a0draw.</p>\n<p>Every player marks a square in turn, usually with a single letter \u201cX\u201d for one player and the letter \"O\" for the other. The goal is to earn three of those points in a row\u200a\u2014\u200avertically, in the horizontal direction, or diagonally\u200a\u2014\u200abefore your rival does\u00a0so</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/225/1*DJz29N7suwDLodoL639Z7g.jpeg\"></figure><h4>MyCobot</h4>\n<p>The myCobot 280 series, created by Elephant Robot, represents a line of 6-degree-of-freedom (6-DOF) collaborative robotic arms designed primarily for research, education, science and technology applications, and commercial exhibitions. Tens of thousands of users worldwide have embraced the convenience and efficiency offered by myCobot arms, utilizing them for learning and implementing robotics across various fields. It has a working radius of 280mm and can handle a payload of\u00a0250g.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/282/0*3hVTrBjtAIi1NW9-.png\"><figcaption>Credits: Elephant\u00a0Robotics</figcaption></figure><ol>\n<li>\n<em>Collaborative Robot (Cobot) Design:</em> Designed as a collaborative robot, myCobot is intended to work alongside humans, fostering a safe and efficient environment for research, education, and other applications.</li>\n<li>\n<em>Research and Education Focus:</em> Tailored specifically for research and educational purposes, myCobot serves as an ideal platform for learning and experimentation in robotics.</li>\n<li>\n<em>Science and Technology Applications: </em>With its advanced technology, myCobot finds utility in a wide range of scientific and technological applications, contributing to various industries.</li>\n<li>\n<em>Commercial Exhibitions:</em> The robot\u2019s capabilities extend to commercial exhibitions, where it can showcase its functionalities and applications to a broader audience.</li>\n</ol>\n<p>Forward Kinematics of\u00a0myCobot</p>\n<a href=\"https://medium.com/media/ec3900d58402741941101b5b9245510b/href\">https://medium.com/media/ec3900d58402741941101b5b9245510b/href</a><p>In 2023, the introduction of the AI Kit (Artificial Intelligence Kit) marks a significant advancement in automation technology. This kit seamlessly integrates visual, positioning, grabbing, and automatic sorting modules to create a comprehensive robotic arm application. Designed to emulate an industrial setting, the AI Kit 2023 revolutionizes manual labor by implementing intelligent sorting and initiating preliminary industrial automation processes.</p>\n<p>At the heart of this simulated industrial scenario are five cutting-edge visual algorithms and motion control algorithms for the robotic arm. These algorithms work in tandem, enabling rapid object recognition and precise sorting capabilities. The AI Kit 2023 represents a leap forward in the pursuit of efficient and smart automation within industrial environments.</p>\n<p>The 5 vision algorithms are:</p>\n<p>\u25cf Shape recognition</p>\n<p>\u25cf Feature point recognition</p>\n<p>\u25cf <strong>ArUco</strong> <strong>code </strong>recognition</p>\n<p>\u25cf Color recognition</p>\n<p>\u25cf <strong>YOLOv5</strong> recognition</p>\n<p>The initial four algorithms of the AI Kit 2023 focus on image processing and machine vision, leveraging OpenCV algorithms. These algorithms encompass color space recognition, feature point recognition, ArUco code recognition, and shape recognition. Each of these components contributes to the kit\u2019s robust visual capabilities, enabling rapid and precise identification of objects within the industrial environment.</p>\n<p>Additionally, the AI Kit 2023 incorporates the YOLOv5 (You Only Look Once version 5) algorithm, recognized as a leading object detection method. YOLOv5 employs Convolutional Neural Networks (CNNs) to predict objects in images swiftly, ensuring rapid detection without compromising on accuracy. As the latest iteration of the YOLO series, version 5 further enhances the AI Kit\u2019s object recognition capabilities, making it a cutting-edge solution for efficient and reliable industrial automation.</p>\n<p>Out of all the above vision algorithms, I tried two (color recognition and YOLOv5) to complete this task. Have a look at the videos for each of the individual.</p>\n<p><strong>Color recognition:</strong></p>\n<p>In this scenario, the AI Kit 2023 operates in an eye-to-hand mode, utilizing a camera and harnessing the power of Python and OpenCV. The system employs OpenCV for color positioning, where it identifies color blocks based on predefined criteria. Once identified, the system frames these color blocks and calculates their relative positions using relevant points within the spatial coordinates of the robotic\u00a0arm.</p>\n<p>The next step involves establishing a set of coordinated actions for the robotic arm, tailored to the specific color and spatial coordinates of the identified objects. The robotic arm then executes these actions, placing the objects in designated areas based on their identified colors. This approach ensures a seamless integration of visual recognition, spatial coordination, and precise manipulation by the robotic arm, facilitating efficient sorting and automation in the industrial setting.</p>\n<a href=\"https://medium.com/media/80894c3a29d5df7a685a4383159d381a/href\">https://medium.com/media/80894c3a29d5df7a685a4383159d381a/href</a><p><strong>YOLO Image Recognition:</strong></p>\n<p>In this application, the AI Kit 2023 employs an eye-to-hand mode, utilizing a camera for image capture. The system utilizes OpenCV to load YOLOv5 model data, facilitating the recognition of image blocks within the captured images. Once identified, the system determines the position of these image blocks within the recognition area, utilizing relevant points for spatial coordinate calculations relative to the robotic\u00a0arm.</p>\n<p>Following recognition and spatial analysis, a predefined set of actions is established for the robotic arm. These actions are tailored to the specific characteristics of the recognized objects. The robotic arm then executes these actions, placing the identified objects into designated areas based on their unique features. This integrated approach seamlessly combines image recognition, spatial coordination, and precise manipulation, showcasing the AI Kit\u2019s ability to automate object recognition and sorting tasks in an industrial context.</p>\n<a href=\"https://medium.com/media/5693ffe42099595884b190a83ed644cd/href\">https://medium.com/media/5693ffe42099595884b190a83ed644cd/href</a><p>Finally, opted to go with the color recognition algorithm, and the setup for this task will be explained in <a href=\"https://medium.com/@karteekmenda93/tic-tac-toe-with-mycobot-part-2-aba39fcf7811\">Part\u00a02</a>.</p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"https://www.linkedin.com/in/karteek-menda/\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=71e9ab2f1432\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["tic-tac-toe","minimax-algorithm","robotics","yolov5"]},{"title":"Principal Component Analysis","pubDate":"2023-12-18 05:50:57","link":"https://medium.com/@karteekmenda93/principal-component-analysis-fcb45cc48f9a?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/fcb45cc48f9a","author":"Karteek Menda","thumbnail":"","description":"\n<p>Hello Aliens\u2026..</p>\n<p>In continuation of my commitment to regular blogging, as indicated in the previous post, I am pleased to present another blog that I believe will be beneficial for the community.</p>\n<p>This blog aims to offer insights into principal component analysis by providing you with valuable information on the\u00a0topic.</p>\n<h4><strong>What does PCA\u00a0do?</strong></h4>\n<p>It seeks to identify the direction that maximizes data variation, effectively explaining a significant amount of information.</p>\n<p><strong>For example: </strong>During a survey conducted at five different houses, we discovered the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/637/1*IcH7Fx0-SElZ7L12GqxvFA.png\"></figure><p>We have \u201c<strong>Number of Noses\u201d</strong> on the <strong>X-axis</strong> and<strong> \u201cNumber of Necks\u201d </strong>on the<strong> Y-axis. </strong>In order to encompass all the information, diverse features such as <strong>(1, 1)</strong>, <strong>(2, 2)</strong>, <strong>(3, 3)</strong>, <strong>(4, 4)</strong>, and <strong>(5, 5)</strong> are utilized. However, by examining these data points from a specific perspective and introducing a new axis, which we refer to as the \u201c<strong>number of people,\"</strong> a more insightful representation is achieved.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/762/1*oihVIMCujng-w-nSvDFGtA.png\"></figure><p>This tells us that <strong>House 1</strong> has <strong>one</strong> person, <strong>House 2</strong> has <strong>two</strong>, and so\u00a0on.</p>\n<p>We effortlessly transformed this two-dimensional data into a one-dimensional format.</p>\n<h4><strong>How does PCA actually\u00a0work?</strong></h4>\n<p>Let\u2019s say we have a dataset of two features.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/411/1*-FM8WEokCsvaO3CUgYiIkQ.png\"></figure><p><strong>Step\u200a\u2014\u200a1:</strong></p>\n<p>Find the centroids of this dataset, which has two features.<br><strong>Centroid = (Mean Value of all X coordinates, Mean value of all Y coordinates)<br>Green-colored</strong> dot indicated above is the <strong>Centroid.</strong></p>\n<p><strong>Step\u200a\u2014\u200a2:</strong></p>\n<p>We move the entire dataset in a way that positions the centroid precisely at the origin (0, 0), all while maintaining the relative direction of each data\u00a0point.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/470/1*FpUITufq7xjeIKWp3IpAnw.png\"></figure><p>Thus, we can effectively relocate the dataset to a condition where the centroid is situated at the\u00a0origin.</p>\n<p><strong>Step\u200a\u2014\u200a3:</strong></p>\n<p>Therefore, we draw multiple lines passing through the origin and identify the line that maximizes its ability to capture information.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/513/1*GvFtfi1_Z17eaGPS941Kzw.png\"></figure><p>Now arises the question: How do we determine the line that maximizes the amount of captured information?</p>\n<p><strong>a.</strong> Imagine that this below is one of the contender lines</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/389/1*qle6eS0kNSnRst5pQH5baQ.png\"></figure><p><strong>b.</strong> Project all the points onto this specific line in a perpendicular manner<br><strong>c.</strong> Compute the Euclidean distance between each projected point and the origin for all the data points<br><strong>d.</strong> Next, square each of these distances and sum them\u00a0up</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/509/1*-FfWrFLi_uQgESyserBauA.png\"></figure><p>Now, we calculate the sum of squared distances for each and every line (Step 3). The line that gives the maximum value for that is principal component 1. And its corresponding Eigan value\u00a0is:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/406/1*xFIMITgEB4n3ni7uVfd4PA.png\"></figure><p>Where <strong>n</strong> is the number of data\u00a0points</p>\n<p><strong>Eigan value</strong> of <strong>PC 1</strong> is <strong>8.59</strong><br>Eigenvalues provide a measure of the extent of variation around the\u00a0line.</p>\n<p><strong>Step\u200a\u2014\u200a4:</strong></p>\n<p><em>How do I find the Eigan vectors?</em><br>Now that we have found the line that maximizes the variation around the central point, we have the equation for that line. So, let's say we have the line equation.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/153/1*YUYYK-GeOkcsVqpp4RvCSA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/681/1*gCkwtuIl51p9D1eIGzmDpQ.png\"></figure><p>By employing the Pythagorean Theorem, we determine the hypotenuse of the right-angled triangle to be <strong>1.47</strong>. Subsequently, to ensure that this specific vector becomes a unit vector, we normalize it by dividing all three sides by <strong>1.47</strong>. Following this adjustment, the new hypotenuse becomes <strong>1</strong>, with the sides measuring <strong>0.68</strong> and <strong>0.73</strong>, denoted as the loading scores of <strong>PC-1</strong>. These loadings indicate the significance of a specific feature for a given principal component, reflecting the importance of that feature in contributing to the principal component\u2019s variation.</p>\n<p>The principal components are oriented orthogonally to each other and intersect at the origin. Consequently, when deriving <strong>PC-2</strong> after obtaining <strong>PC-1</strong> in a two-dimensional space with features X and Y, <strong>PC-2</strong> represents the perpendicular line to <strong>PC-1</strong>, ensuring that both components pass through the\u00a0origin.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/468/1*TjbVJofTpRPEYRA9O_YzvQ.png\"></figure><p>So, the equation of the line (<strong>PC-2</strong>)\u00a0is</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/176/1*IXWKBTbo2pzXJoYuynsMMw.png\"></figure><p><strong>Eigan Value</strong> of <strong>PC-2</strong> is\u00a0<strong>0.46</strong></p>\n<p>The principal components are essentially linear combinations of the original features. For instance, if there are five distinct features, there would be five principal components, each perpendicular to the others. This orthogonal arrangement resolves the issue of multicollinearity, ensuring that the principal components are linearly independent.<br><strong>Recall:</strong> <strong>Eigan value</strong> of <strong>PC-1</strong> is <strong>8.59\u00a0</strong>, signifying that a larger proportion of information is encapsulated in <strong>PC- 1</strong>, whereas <strong>PC-2</strong> captures a relatively smaller amount of information. Consequently, the eigenvalue and corresponding principal component value are diminished for\u00a0<strong>PC-2</strong>.</p>\n<p>To examine the individual contributions of eigenvalues<br>for\u00a0<strong>PC-1</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/501/1*barKs52GgS_dOvGnITgtpg.png\"></figure><p>for <strong>PC-2</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/486/1*eQklAppvMGzbWhRVL2uPTA.png\"></figure><p>Lets plot this to see the variation captured by each of the two\u00a0<strong>PC</strong>\u2019s</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/464/1*eHHRo0khZLOy8Yd0rJgyeQ.png\"></figure><p>Based on the depicted plot, we can infer that utilizing only PC1 allows us to capture 95 percent of the total variation in the\u00a0data.</p>\n<p><strong>Advantages of PCA:<br>1. </strong>Improves algorithmic performance<strong><br>2. </strong>Reduction of dimension of data<strong><br>3. </strong>Removes correlated features<strong><br>4. </strong>Reduces high variance (Overfitting)<strong><br>5. </strong>Improves visualization</p>\n<p><strong>Disadvantages of PCA:<br>1. </strong>Less Interpretable<strong><br>2. </strong>Loss of information<strong><br>3. </strong>PCA is a linear dimensionality reduction technique, but not all real-world datasets may be\u00a0linear</p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fcb45cc48f9a\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Hello Aliens\u2026..</p>\n<p>In continuation of my commitment to regular blogging, as indicated in the previous post, I am pleased to present another blog that I believe will be beneficial for the community.</p>\n<p>This blog aims to offer insights into principal component analysis by providing you with valuable information on the\u00a0topic.</p>\n<h4><strong>What does PCA\u00a0do?</strong></h4>\n<p>It seeks to identify the direction that maximizes data variation, effectively explaining a significant amount of information.</p>\n<p><strong>For example: </strong>During a survey conducted at five different houses, we discovered the following:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/637/1*IcH7Fx0-SElZ7L12GqxvFA.png\"></figure><p>We have \u201c<strong>Number of Noses\u201d</strong> on the <strong>X-axis</strong> and<strong> \u201cNumber of Necks\u201d </strong>on the<strong> Y-axis. </strong>In order to encompass all the information, diverse features such as <strong>(1, 1)</strong>, <strong>(2, 2)</strong>, <strong>(3, 3)</strong>, <strong>(4, 4)</strong>, and <strong>(5, 5)</strong> are utilized. However, by examining these data points from a specific perspective and introducing a new axis, which we refer to as the \u201c<strong>number of people,\"</strong> a more insightful representation is achieved.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/762/1*oihVIMCujng-w-nSvDFGtA.png\"></figure><p>This tells us that <strong>House 1</strong> has <strong>one</strong> person, <strong>House 2</strong> has <strong>two</strong>, and so\u00a0on.</p>\n<p>We effortlessly transformed this two-dimensional data into a one-dimensional format.</p>\n<h4><strong>How does PCA actually\u00a0work?</strong></h4>\n<p>Let\u2019s say we have a dataset of two features.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/411/1*-FM8WEokCsvaO3CUgYiIkQ.png\"></figure><p><strong>Step\u200a\u2014\u200a1:</strong></p>\n<p>Find the centroids of this dataset, which has two features.<br><strong>Centroid = (Mean Value of all X coordinates, Mean value of all Y coordinates)<br>Green-colored</strong> dot indicated above is the <strong>Centroid.</strong></p>\n<p><strong>Step\u200a\u2014\u200a2:</strong></p>\n<p>We move the entire dataset in a way that positions the centroid precisely at the origin (0, 0), all while maintaining the relative direction of each data\u00a0point.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/470/1*FpUITufq7xjeIKWp3IpAnw.png\"></figure><p>Thus, we can effectively relocate the dataset to a condition where the centroid is situated at the\u00a0origin.</p>\n<p><strong>Step\u200a\u2014\u200a3:</strong></p>\n<p>Therefore, we draw multiple lines passing through the origin and identify the line that maximizes its ability to capture information.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/513/1*GvFtfi1_Z17eaGPS941Kzw.png\"></figure><p>Now arises the question: How do we determine the line that maximizes the amount of captured information?</p>\n<p><strong>a.</strong> Imagine that this below is one of the contender lines</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/389/1*qle6eS0kNSnRst5pQH5baQ.png\"></figure><p><strong>b.</strong> Project all the points onto this specific line in a perpendicular manner<br><strong>c.</strong> Compute the Euclidean distance between each projected point and the origin for all the data points<br><strong>d.</strong> Next, square each of these distances and sum them\u00a0up</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/509/1*-FfWrFLi_uQgESyserBauA.png\"></figure><p>Now, we calculate the sum of squared distances for each and every line (Step 3). The line that gives the maximum value for that is principal component 1. And its corresponding Eigan value\u00a0is:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/406/1*xFIMITgEB4n3ni7uVfd4PA.png\"></figure><p>Where <strong>n</strong> is the number of data\u00a0points</p>\n<p><strong>Eigan value</strong> of <strong>PC 1</strong> is <strong>8.59</strong><br>Eigenvalues provide a measure of the extent of variation around the\u00a0line.</p>\n<p><strong>Step\u200a\u2014\u200a4:</strong></p>\n<p><em>How do I find the Eigan vectors?</em><br>Now that we have found the line that maximizes the variation around the central point, we have the equation for that line. So, let's say we have the line equation.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/153/1*YUYYK-GeOkcsVqpp4RvCSA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/681/1*gCkwtuIl51p9D1eIGzmDpQ.png\"></figure><p>By employing the Pythagorean Theorem, we determine the hypotenuse of the right-angled triangle to be <strong>1.47</strong>. Subsequently, to ensure that this specific vector becomes a unit vector, we normalize it by dividing all three sides by <strong>1.47</strong>. Following this adjustment, the new hypotenuse becomes <strong>1</strong>, with the sides measuring <strong>0.68</strong> and <strong>0.73</strong>, denoted as the loading scores of <strong>PC-1</strong>. These loadings indicate the significance of a specific feature for a given principal component, reflecting the importance of that feature in contributing to the principal component\u2019s variation.</p>\n<p>The principal components are oriented orthogonally to each other and intersect at the origin. Consequently, when deriving <strong>PC-2</strong> after obtaining <strong>PC-1</strong> in a two-dimensional space with features X and Y, <strong>PC-2</strong> represents the perpendicular line to <strong>PC-1</strong>, ensuring that both components pass through the\u00a0origin.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/468/1*TjbVJofTpRPEYRA9O_YzvQ.png\"></figure><p>So, the equation of the line (<strong>PC-2</strong>)\u00a0is</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/176/1*IXWKBTbo2pzXJoYuynsMMw.png\"></figure><p><strong>Eigan Value</strong> of <strong>PC-2</strong> is\u00a0<strong>0.46</strong></p>\n<p>The principal components are essentially linear combinations of the original features. For instance, if there are five distinct features, there would be five principal components, each perpendicular to the others. This orthogonal arrangement resolves the issue of multicollinearity, ensuring that the principal components are linearly independent.<br><strong>Recall:</strong> <strong>Eigan value</strong> of <strong>PC-1</strong> is <strong>8.59\u00a0</strong>, signifying that a larger proportion of information is encapsulated in <strong>PC- 1</strong>, whereas <strong>PC-2</strong> captures a relatively smaller amount of information. Consequently, the eigenvalue and corresponding principal component value are diminished for\u00a0<strong>PC-2</strong>.</p>\n<p>To examine the individual contributions of eigenvalues<br>for\u00a0<strong>PC-1</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/501/1*barKs52GgS_dOvGnITgtpg.png\"></figure><p>for <strong>PC-2</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/486/1*eQklAppvMGzbWhRVL2uPTA.png\"></figure><p>Lets plot this to see the variation captured by each of the two\u00a0<strong>PC</strong>\u2019s</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/464/1*eHHRo0khZLOy8Yd0rJgyeQ.png\"></figure><p>Based on the depicted plot, we can infer that utilizing only PC1 allows us to capture 95 percent of the total variation in the\u00a0data.</p>\n<p><strong>Advantages of PCA:<br>1. </strong>Improves algorithmic performance<strong><br>2. </strong>Reduction of dimension of data<strong><br>3. </strong>Removes correlated features<strong><br>4. </strong>Reduces high variance (Overfitting)<strong><br>5. </strong>Improves visualization</p>\n<p><strong>Disadvantages of PCA:<br>1. </strong>Less Interpretable<strong><br>2. </strong>Loss of information<strong><br>3. </strong>PCA is a linear dimensionality reduction technique, but not all real-world datasets may be\u00a0linear</p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fcb45cc48f9a\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["pca-analysis","principal-component","eigenvalue","machine-learning","eigenvectors"]},{"title":"How does Google Maps predict the ETA with high precision?","pubDate":"2023-12-15 19:23:21","link":"https://medium.com/@karteekmenda93/how-does-google-maps-predict-the-eta-with-high-precision-b9bac345fd4?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/b9bac345fd4","author":"Karteek Menda","thumbnail":"","description":"\n<p>Hello Aliens\u2026..</p>\n<p>It\u2019s been some time since my last blog, as I am busy with my work and studies. Now that I have had some good time, I will be regularly posting the blogs from now\u00a0on.</p>\n<p>In this blog, I will try to give you imformation about how Google Maps estimates the\u00a0time.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yOVJaQqFpZU5dDy6_nlGuQ.png\"><figcaption>Source: Google\u00a0Maps</figcaption></figure><p>Let's say I want a route from <strong>Tempe Lake</strong> to <strong>Phoenix-Mesa Gateway Airport</strong>. Google estimates that it will take around 33 minutes to reach the destination. And how does it do\u00a0this?</p>\n<p>So, we can break this route down into road segments, and we need to find the time taken to traverse each road segment and sum them up in order to get the\u00a0ETA.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*U1LqFqW705-q_0j1jRxhtQ.jpeg\"></figure><p>What is a <em>possible solution</em> for\u00a0this?</p>\n<h4><strong>Possible Solution\u00a01:</strong></h4>\n<p>Maybe we can segment the entire route into road segments. And traverse each road segment and sum them up to get the ETA. So, the solution could be using a <strong>Feed Forward Neural Network</strong>. The input feed can be the encoded representation of that road segment, and the output can be a single neuron that gives time to traverse. Likewise, I can pass through all the road segments and get the time to traverse each of the road segments. At last, we can sum those times to get the ETA of the\u00a0total.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AHyioAx-VTcas_A5ovllRg.jpeg\"></figure><p><em>Downsides of the above solution: Even though the above solution sounds like an easy solution, there is a problem here. Feed Forward Neural Networks require the samples to be independent of each other. But with road segments, traffic on one road can easily influence the traffic and ETA of the neighbouring roads, which clearly indicates that there is some dependence between the\u00a0samples.</em></p>\n<h4>Possible Solution\u00a02:</h4>\n<p>As the road segments are dependent on each other, we can use a network that can handle sequence data and parallelization. So, <a href=\"https://medium.com/@karteekmenda93/part-1-sentimental-analysis-using-bert-c030ca9b33b6\"><strong>Transformers</strong></a><strong> </strong>comes to our rescue. In the transformers, we could potentially use the encoder. Feed in the road segments simultaneously and get their corresponding embedding vectors simultaneously. And pass each embedding vector through the Feed Forward Neural Network, get the ETA for each of those road segments, and sum them up to get the total\u00a0ETA.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rUDTBOoZNnffnyF3qNPc6g.jpeg\"></figure><p><em>Downsides of the above solution: We need a lot of data. Transformers have no understanding of how road segments are related to each other. It needs to learn this relationship from scratch with a ton of examples. In order to get an understanding of this, transformers are going to take a lot of compute power, a lot of time, and a lot of\u00a0data.</em></p>\n<p><strong>But we have arrived at the point where roads adjacent to each other are more related than roads that are nowhere near connected.</strong></p>\n<h4>Possible Solution\u00a03:</h4>\n<p>The most intuitive way to surface this knowledge of roads is with <strong>Graphs. </strong>So<strong> </strong>graphs consist of vertices and edges. In this context, let us consider every road segment as a vertex, and if two road segments are connected to each other, their corresponding nodes will be connected by an edge. So we can essentially create a graph for all of Tempe or any section of any city that you\u00a0want.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RlBul__XRj11S-mpj-Graw.jpeg\"></figure><p>The goal now is to learn the embeddings for the nodes, i.e., the road segments, and this is done by an iterative process called <strong>Message\u00a0Passing.</strong></p>\n<p><em>Let me give you a simpler idea of what a </em><strong><em>message passing</em></strong><em>\u00a0is.</em></p>\n<p>Consider a graph with just two nodes A and B, which points to a third node C. We are going to do message passing to C. This involves two\u00a0steps.</p>\n<ol>\n<li>\n<strong>Creating the message</strong> for node C involves taking the inbound nodes to C and applying some operation (addition of A and B) to the\u00a0vectors.</li>\n<li>\n<strong>Update State</strong>\u00a0: Now we take the message vector from (1), take the current vector C and pass it into another function (addition) to get the new state of C. In the end, we just need the output of a vector that is representative of the embeddings of\u00a0C.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FcSPb-HTH_aQVTMSE03egw.jpeg\"></figure><p>But the summation operations we are using here might be a little too weak to actually get the embeddings that we\u00a0need.</p>\n<p>So, we can potentially use transformers instead. We can take two vectors, that is, nodes A and B. Also, we can take the information about the connecting edges, which would be AC and BC. Feed all of these embeddings into the transformer encoder. And we get the encodings of these vectors simultaneously. Next, they are used by the transformer decoder along with the current node vector C in order to get the next state (new state) of Vector C. This is message passing for node C and only for one-time\u00a0step.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CU8iMi-8xiFHMpZT5AMeyA.jpeg\"></figure><p>This process happens simultaneously for all the nodes in the graph network because each node's next state is dependent on its previous state. In this way, every node's neighbours are encoded into its embeddings, and we can execute this multiple times to get better and better embeddings.</p>\n<p>So, on the first go of message passing, every node knows about its neighbors, and in the second iteration, every node knows about its neighbor\u2019s neighbor. And in the third iteration, even their neighbors are encoded into every node\u2019s embedding, and so on. In this context, after <em>n</em> passes of message passing, we have the road network embeddings.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5JIa_ctpHH88-FIZsyiIcw.jpeg\"></figure><p>Now, since the orientation of the roads doesn't change much, we can store these road embeddings in a <strong>look-up dictionary</strong> that we can use during inference time.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WAnsJCLjwj7bxdApkCSEIQ.jpeg\"></figure><p>Now these road vectors contain information about the location of the road itself and its relationship with the other roads. But we might need some more additional information to get more accurate results, such as traffic information, speed level information, and accidents. All of this can be incorporated too.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tiEq1eFysbaLaWylDY9gQg.jpeg\"></figure><p>This is where we pass all of this information into a feed-forward neural network and get the most up-to-date time to traverse that\u00a0segment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8OCl8h_s6xPQyfxT09AdLg.jpeg\"></figure><p>So to summarize, the prediction of the ETA\u00a0is:</p>\n<ol>\n<li>Determine the start and end points on your\u00a0map.</li>\n<li>Use some algorithm to find a\u00a0path.</li>\n<li>To determine the road segments along the\u00a0path.</li>\n<li>During the training phase, we could have stored all the road segments and embeddings into a <strong>look-up dictionary</strong>. So, during inference, all we need to do is just reference each road segment by that ID to that dictionary store and get the corresponding road segment embeddings.</li>\n<li>Query some real-time features of traffic, accidents, and speed limits and pass them all along with the embedding vector into a feed-forward neural network to get the expected time of traversing that\u00a0segment.</li>\n<li>Sum all of the times to get the ETA from start to\u00a0finish.</li>\n</ol>\n<p>I would strongly recommend that the Aliens visit <a href=\"https://medium.com/@karteekmenda93/part-1-sentimental-analysis-using-bert-c030ca9b33b6\">Part 1: Sentimental Analysis Using BERT</a> to get an understanding of the transformer architecture in detail. I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, and deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p><strong>Happy Learning\u2026\u2026.</strong></p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b9bac345fd4\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Hello Aliens\u2026..</p>\n<p>It\u2019s been some time since my last blog, as I am busy with my work and studies. Now that I have had some good time, I will be regularly posting the blogs from now\u00a0on.</p>\n<p>In this blog, I will try to give you imformation about how Google Maps estimates the\u00a0time.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*yOVJaQqFpZU5dDy6_nlGuQ.png\"><figcaption>Source: Google\u00a0Maps</figcaption></figure><p>Let's say I want a route from <strong>Tempe Lake</strong> to <strong>Phoenix-Mesa Gateway Airport</strong>. Google estimates that it will take around 33 minutes to reach the destination. And how does it do\u00a0this?</p>\n<p>So, we can break this route down into road segments, and we need to find the time taken to traverse each road segment and sum them up in order to get the\u00a0ETA.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*U1LqFqW705-q_0j1jRxhtQ.jpeg\"></figure><p>What is a <em>possible solution</em> for\u00a0this?</p>\n<h4><strong>Possible Solution\u00a01:</strong></h4>\n<p>Maybe we can segment the entire route into road segments. And traverse each road segment and sum them up to get the ETA. So, the solution could be using a <strong>Feed Forward Neural Network</strong>. The input feed can be the encoded representation of that road segment, and the output can be a single neuron that gives time to traverse. Likewise, I can pass through all the road segments and get the time to traverse each of the road segments. At last, we can sum those times to get the ETA of the\u00a0total.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AHyioAx-VTcas_A5ovllRg.jpeg\"></figure><p><em>Downsides of the above solution: Even though the above solution sounds like an easy solution, there is a problem here. Feed Forward Neural Networks require the samples to be independent of each other. But with road segments, traffic on one road can easily influence the traffic and ETA of the neighbouring roads, which clearly indicates that there is some dependence between the\u00a0samples.</em></p>\n<h4>Possible Solution\u00a02:</h4>\n<p>As the road segments are dependent on each other, we can use a network that can handle sequence data and parallelization. So, <a href=\"https://medium.com/@karteekmenda93/part-1-sentimental-analysis-using-bert-c030ca9b33b6\"><strong>Transformers</strong></a><strong> </strong>comes to our rescue. In the transformers, we could potentially use the encoder. Feed in the road segments simultaneously and get their corresponding embedding vectors simultaneously. And pass each embedding vector through the Feed Forward Neural Network, get the ETA for each of those road segments, and sum them up to get the total\u00a0ETA.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*rUDTBOoZNnffnyF3qNPc6g.jpeg\"></figure><p><em>Downsides of the above solution: We need a lot of data. Transformers have no understanding of how road segments are related to each other. It needs to learn this relationship from scratch with a ton of examples. In order to get an understanding of this, transformers are going to take a lot of compute power, a lot of time, and a lot of\u00a0data.</em></p>\n<p><strong>But we have arrived at the point where roads adjacent to each other are more related than roads that are nowhere near connected.</strong></p>\n<h4>Possible Solution\u00a03:</h4>\n<p>The most intuitive way to surface this knowledge of roads is with <strong>Graphs. </strong>So<strong> </strong>graphs consist of vertices and edges. In this context, let us consider every road segment as a vertex, and if two road segments are connected to each other, their corresponding nodes will be connected by an edge. So we can essentially create a graph for all of Tempe or any section of any city that you\u00a0want.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*RlBul__XRj11S-mpj-Graw.jpeg\"></figure><p>The goal now is to learn the embeddings for the nodes, i.e., the road segments, and this is done by an iterative process called <strong>Message\u00a0Passing.</strong></p>\n<p><em>Let me give you a simpler idea of what a </em><strong><em>message passing</em></strong><em>\u00a0is.</em></p>\n<p>Consider a graph with just two nodes A and B, which points to a third node C. We are going to do message passing to C. This involves two\u00a0steps.</p>\n<ol>\n<li>\n<strong>Creating the message</strong> for node C involves taking the inbound nodes to C and applying some operation (addition of A and B) to the\u00a0vectors.</li>\n<li>\n<strong>Update State</strong>\u00a0: Now we take the message vector from (1), take the current vector C and pass it into another function (addition) to get the new state of C. In the end, we just need the output of a vector that is representative of the embeddings of\u00a0C.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FcSPb-HTH_aQVTMSE03egw.jpeg\"></figure><p>But the summation operations we are using here might be a little too weak to actually get the embeddings that we\u00a0need.</p>\n<p>So, we can potentially use transformers instead. We can take two vectors, that is, nodes A and B. Also, we can take the information about the connecting edges, which would be AC and BC. Feed all of these embeddings into the transformer encoder. And we get the encodings of these vectors simultaneously. Next, they are used by the transformer decoder along with the current node vector C in order to get the next state (new state) of Vector C. This is message passing for node C and only for one-time\u00a0step.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*CU8iMi-8xiFHMpZT5AMeyA.jpeg\"></figure><p>This process happens simultaneously for all the nodes in the graph network because each node's next state is dependent on its previous state. In this way, every node's neighbours are encoded into its embeddings, and we can execute this multiple times to get better and better embeddings.</p>\n<p>So, on the first go of message passing, every node knows about its neighbors, and in the second iteration, every node knows about its neighbor\u2019s neighbor. And in the third iteration, even their neighbors are encoded into every node\u2019s embedding, and so on. In this context, after <em>n</em> passes of message passing, we have the road network embeddings.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5JIa_ctpHH88-FIZsyiIcw.jpeg\"></figure><p>Now, since the orientation of the roads doesn't change much, we can store these road embeddings in a <strong>look-up dictionary</strong> that we can use during inference time.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*WAnsJCLjwj7bxdApkCSEIQ.jpeg\"></figure><p>Now these road vectors contain information about the location of the road itself and its relationship with the other roads. But we might need some more additional information to get more accurate results, such as traffic information, speed level information, and accidents. All of this can be incorporated too.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tiEq1eFysbaLaWylDY9gQg.jpeg\"></figure><p>This is where we pass all of this information into a feed-forward neural network and get the most up-to-date time to traverse that\u00a0segment.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8OCl8h_s6xPQyfxT09AdLg.jpeg\"></figure><p>So to summarize, the prediction of the ETA\u00a0is:</p>\n<ol>\n<li>Determine the start and end points on your\u00a0map.</li>\n<li>Use some algorithm to find a\u00a0path.</li>\n<li>To determine the road segments along the\u00a0path.</li>\n<li>During the training phase, we could have stored all the road segments and embeddings into a <strong>look-up dictionary</strong>. So, during inference, all we need to do is just reference each road segment by that ID to that dictionary store and get the corresponding road segment embeddings.</li>\n<li>Query some real-time features of traffic, accidents, and speed limits and pass them all along with the embedding vector into a feed-forward neural network to get the expected time of traversing that\u00a0segment.</li>\n<li>Sum all of the times to get the ETA from start to\u00a0finish.</li>\n</ol>\n<p>I would strongly recommend that the Aliens visit <a href=\"https://medium.com/@karteekmenda93/part-1-sentimental-analysis-using-bert-c030ca9b33b6\">Part 1: Sentimental Analysis Using BERT</a> to get an understanding of the transformer architecture in detail. I plan to share additional blog posts covering topics such as robotics, drive-by-wire vehicles, machine learning, and deep learning, etc..</p>\n<p><strong>Stay tuned</strong>.</p>\n<p><strong>Happy Learning\u2026\u2026.</strong></p>\n<p><strong>Thanks for</strong> reading the article! If you <strong>like</strong> my article, do \ud83d\udc4f this article. If you want to <strong>connect</strong> with me on <strong>LinkedIn</strong>, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>This is <strong>Karteek\u00a0Menda</strong>.</p>\n<p><strong>Signing Off</strong></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b9bac345fd4\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["graph-neural-networks","google-maps","message-passing","feed-forward-networks","transformers"]},{"title":"Part 1: Sentimental Analysis Using BERT","pubDate":"2021-07-10 10:48:16","link":"https://medium.com/@karteekmenda93/part-1-sentimental-analysis-using-bert-c030ca9b33b6?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/c030ca9b33b6","author":"Karteek Menda","thumbnail":"","description":"\n<p>Hello Aliens\u2026..</p>\n<p>Sentiments are those which are a huge asset to the company\u2019s now a days. The company\u2019s can know the emotional tone of the reviews or else some feedback comments etc. This will give the company the entire picture of what people are thinking about their company or else their products.</p>\n<p><strong><em>Sentiment analysis</em></strong> is extremely useful in social media monitoring because it allows us to gain a broader overview of the public opinion behind some particular topics. The applications of sentiment analysis are broad and most powerful. The ability to extract insights from social data is a practice that is being widely adopted by organisations across the world, and it proved to be most helpful for them to plan accordingly.</p>\n<p>Let me throw some light on how well this sentiment analysis has helped the Obama administration in their 2012 presidential election campaign. The Obama administration has used sentiment analysis to know the public opinion to policy announcements and campaign messages ahead of 2012 presidential election. Team Obama could be able to quickly see the sentiment behind everything from forum posts to news articles and being better to strategize and plan for the\u00a0future.</p>\n<p>Expedia Canada took advantage of sentiment analysis very quickly and reacted accordingly when they noticed that there was a steady increase in negative feedback to the music used in one of their television advertisements.</p>\n<p>Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis tools allow businesses to identify customer sentiment toward products, brands or services in online feedback. A sentiment analysis system for text analysis combines natural language processing (<a href=\"https://www.lexalytics.com/lexablog/what-is-natural-language-processing\">NLP</a>) and machine learning techniques to assign weighted sentiment scores to the entities, topics, themes and categories within a sentence or\u00a0phrase.</p>\n<h3>TRANSFORMERS</h3>\n<p><strong><em>Transformers</em></strong> have come to parallelize the sequential data. The architecture of transformers is much like RNN and the main difference is sequence can be passed in parallel.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*bkOPd0HBJ-rWDrYR.jpeg\"><figcaption>Fig:1 Transformers Architecture. Source: kazemnejad.com</figcaption></figure><p><strong>ENCODER:</strong></p>\n<p>With RNN encoder, we pass an input sentence one word after the other. The current word\u2019s hidden state has dependencies in previous word hidden state. The word embeddings are generated one time step at a time. Whereas, with a transformer encoder, there is no concept of time step for the input. We pass all the words in the sentence simultaneously and determine the word embeddings.</p>\n<p><strong><em>Positional encoder:</em></strong> Vectors that gives context based on position of word in sentence. Generally we pass the input embeddings of a word and then apply positional encoding in a sentence, we get word vectors which have a positional info i.e\u00a0context.</p>\n<p>We pass this into encoder block where it goes through a multi headed attention layer and a feed forward\u00a0layer.</p>\n<p><strong>DECODER:</strong></p>\n<p>We feed the output sentence to the decoder, now we get the input embeddings which gets the vector form of the word and then we add positional vector to get the notion of context of the word in a sentence. We finally pass this resultant vector into a decoder block which has three main components.</p>\n<ol>\n<li>Self attention block generates attention vectors for every word in the sentence to represent how much each word is related to every other word in the same sentence.</li>\n<li>These attention vectors and vectors from the encoders are passed into another attention block called encoder-decoder attention. This attention block will determine how related each word vector with respect to other. And this is where the mapping happens. the output is the attention vectors for every word in the input sentence and the output sentence.</li>\n<li>Next we pass each attention vector feed forward unit. Then, to linear layer(which is a Feed Forward layer). Then, softmax layer translates into a probability distribution which is now human interpretable and the final word is the word correspondent to the highest probability.</li>\n</ol>\n<p>If we stack the blocks to the encoder block, we get the BERT architecture and if the same stacking is done to decoder block, we get the GPT architecture.</p>\n<p>With this knowledge on transformers we can move on to\u00a0BERT.</p>\n<h3>BERT: Bidirectional Encoder Representations from Transformers</h3>\n<p>It is used to solve problems like Machine Translation, Question Answering, Sentimental Analysis, Text Summarization and many more. All of these problems requires the understanding of the language. So, we have two main\u00a0tasks.</p>\n<ol>\n<li>Pre Training BERT to understand the language.</li>\n<li>Fine Tuning BERT to learn specific\u00a0task.</li>\n</ol>\n<p><strong>PRE TRAINING:</strong></p>\n<p>The goal of pre training is to make learn BERT what is language and also what is the context. Basically BERT learns language by training on two unsupervised tasks simultaneously. They\u00a0are</p>\n<p><strong>a) Masked Language Modelling(MLM)</strong></p>\n<p><strong>b) Next Sentence Prediction(NSP)</strong></p>\n<p>For MLM, BERT takes in a sentence and masks some words randomly. The goal is to find these word tokens which are masked(like fill in the blanks). It helps BERT understand bidirectional context within a sentence.</p>\n<p>Example: The fox [MASK 1] over the [MASK 2]\u00a0dog.</p>\n<p>MASK 1\u200a\u2014\u200aJumped.</p>\n<p>MASK 2\u200a\u2014\u200alazy.</p>\n<p>In case of Next Sentence Prediction(NSP), BERT takes in the sentences and determines which sentence comes after the other. Because of this, BERT understands the context across different sentences themselves.</p>\n<p>Example:</p>\n<p>Sent 1\u200a\u2014\u200aCristiano Ronaldo is a goal\u00a0machine.</p>\n<p>Sent 2\u200a\u2014\u200aHe is the GOAT(Greatest Of All\u00a0Time).</p>\n<p>Here Sent 1 followed by Sent\u00a02.</p>\n<p>Now, using both(MLM, NSP) BERT gets a good understanding of the language. Generally these two are trained simultaneously in practise.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/740/0*bDk_-2EUFpDrkRvR.png\"><figcaption>Fig 2</figcaption></figure><p>The input is a set of 2 sentences with some of the words being masked. Each token is a word and then we convert each of these tokens into <strong><em>embeddings</em></strong> using pretrained embeddings. This provides a good starting point for BERT to work\u00a0with.</p>\n<p>During Pre Training, in the input side, how are we going to generate the embeddings from the word token\u00a0inputs?</p>\n<p>Think on it Aliens\u2026\u2026\u2026\u2026\u2026\u2026</p>\n<p>Here comes the answer for that, These initial embeddings constructed from a combination of three kinds of embeddings.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*1sFmGbb-3t1WnB-A.png\"><figcaption>Fig 3: Contextualized word embeddings</figcaption></figure><p><strong>Token Embeddings:</strong> These are the pre trained embeddings. (Word Pieces Embeddings of vocabulary size\u00a030K).</p>\n<p><strong>Sentence Embeddings:</strong> It is nothing but the sentence number that is encoded into a\u00a0vector.</p>\n<p><strong>Position Embeddings:</strong> It is the position of a word within the sentence that is encoded into a\u00a0vector.</p>\n<p>Adding these three vectors together we get an embedding vector that we use as an input to BERT. The sentence and position embeddings are required for temporal ordering since all these vectors are fed in simultaneously into BERT and language modelling needs this ordering preserved.</p>\n<p>Now the output side, the output is a binary value(C) and a bunch of word vectors. \u2018C\u2019 is the binary output for the NSP. So, if C is 1 Sentence B follows Sentence A and C is 0 if Sentence A follows Sentence B. Each of the T\u2019s in the image are the word vectors that corresponds to the outputs for the masked language model problem.But with training, we need to minimize the loss. Here two key details to note. All of the word vectors(T\u2019s) have the same size and all of them are generated simultaneously.</p>\n<p>We need to take each word vector and pass it to a fully connected layered output with the same number of neurons equals to the number of tokens in vocabulary(i.e 30K). We apply a softmax activation. This way we can convert a word vector to a distribution and the actual label of this distribution would be an one hot encoded vector for the actual word. So, we compare these two distributions and train the network using cross entropy loss. Here the loss will be calculated for [MASK]ed words only. This is done to keep more focus on predicting the masked values so that it gets them correct and it tries to improve the\u00a0context.</p>\n<p><strong>FINE TUNING:</strong></p>\n<p>How to use language for specific tasks. For example Question Answering task.</p>\n<p>All we need to do is to replace the fully connected output layers of the network with a fresh set of output layers that can basically give the answer to the question. Then we can perform supervised training using a question answering data set. It won\u2019t take long since it is only the output parameters that are learnt from the scratch. The rest of the model parameters are just slightly fine tuned. As a result, training time is\u00a0fast.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/722/0*Pnw03rxPQyLfXG7E.png\"><figcaption>Fig:4 Illustrations of Fine-tuning BERT on Different Tasks</figcaption></figure><p>In the case of Question Answering(Refer to Fig 4), we would train the model by modifying the inputs and the output layer. We pass in the question followed by a passage containing the answer as input. And the output would be start and end words that encapsulate the answer. Assuming that the answer is within the same span of\u00a0text.</p>\n<p>Performance is dependent on how big we want BERT to be. BERT Large Model which have 340 Million parameters can achieve a way higher accuracy than the BERT base model which has only 110 Million parameters.</p>\n<p>I would strongly recommend the Aliens to visit <a href=\"https://huggingface.co/\">HUGGING FACE</a>: State-of-the-Art Natural Language Processing.</p>\n<p>In <strong>Part 2</strong> of this, we will be going to find the sentiment of the champions league finale(FC Bayern Munich vs PSG). For that, we need to do the\u00a0below:</p>\n<ol>\n<li>Scrape the data from\u00a0twitter.</li>\n<li>Text Preprocessing.</li>\n<li>Applying the BERT model on\u00a0it.</li>\n<li>Getting the polarity score and predicting the sentiments.</li>\n</ol>\n<p>Happy Learning\u2026\u2026.</p>\n<p>Thanks for reading the article! If you like my article do \ud83d\udc4f this article. If you want to connect with me in Linkedin, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c030ca9b33b6\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Hello Aliens\u2026..</p>\n<p>Sentiments are those which are a huge asset to the company\u2019s now a days. The company\u2019s can know the emotional tone of the reviews or else some feedback comments etc. This will give the company the entire picture of what people are thinking about their company or else their products.</p>\n<p><strong><em>Sentiment analysis</em></strong> is extremely useful in social media monitoring because it allows us to gain a broader overview of the public opinion behind some particular topics. The applications of sentiment analysis are broad and most powerful. The ability to extract insights from social data is a practice that is being widely adopted by organisations across the world, and it proved to be most helpful for them to plan accordingly.</p>\n<p>Let me throw some light on how well this sentiment analysis has helped the Obama administration in their 2012 presidential election campaign. The Obama administration has used sentiment analysis to know the public opinion to policy announcements and campaign messages ahead of 2012 presidential election. Team Obama could be able to quickly see the sentiment behind everything from forum posts to news articles and being better to strategize and plan for the\u00a0future.</p>\n<p>Expedia Canada took advantage of sentiment analysis very quickly and reacted accordingly when they noticed that there was a steady increase in negative feedback to the music used in one of their television advertisements.</p>\n<p>Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis tools allow businesses to identify customer sentiment toward products, brands or services in online feedback. A sentiment analysis system for text analysis combines natural language processing (<a href=\"https://www.lexalytics.com/lexablog/what-is-natural-language-processing\">NLP</a>) and machine learning techniques to assign weighted sentiment scores to the entities, topics, themes and categories within a sentence or\u00a0phrase.</p>\n<h3>TRANSFORMERS</h3>\n<p><strong><em>Transformers</em></strong> have come to parallelize the sequential data. The architecture of transformers is much like RNN and the main difference is sequence can be passed in parallel.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*bkOPd0HBJ-rWDrYR.jpeg\"><figcaption>Fig:1 Transformers Architecture. Source: kazemnejad.com</figcaption></figure><p><strong>ENCODER:</strong></p>\n<p>With RNN encoder, we pass an input sentence one word after the other. The current word\u2019s hidden state has dependencies in previous word hidden state. The word embeddings are generated one time step at a time. Whereas, with a transformer encoder, there is no concept of time step for the input. We pass all the words in the sentence simultaneously and determine the word embeddings.</p>\n<p><strong><em>Positional encoder:</em></strong> Vectors that gives context based on position of word in sentence. Generally we pass the input embeddings of a word and then apply positional encoding in a sentence, we get word vectors which have a positional info i.e\u00a0context.</p>\n<p>We pass this into encoder block where it goes through a multi headed attention layer and a feed forward\u00a0layer.</p>\n<p><strong>DECODER:</strong></p>\n<p>We feed the output sentence to the decoder, now we get the input embeddings which gets the vector form of the word and then we add positional vector to get the notion of context of the word in a sentence. We finally pass this resultant vector into a decoder block which has three main components.</p>\n<ol>\n<li>Self attention block generates attention vectors for every word in the sentence to represent how much each word is related to every other word in the same sentence.</li>\n<li>These attention vectors and vectors from the encoders are passed into another attention block called encoder-decoder attention. This attention block will determine how related each word vector with respect to other. And this is where the mapping happens. the output is the attention vectors for every word in the input sentence and the output sentence.</li>\n<li>Next we pass each attention vector feed forward unit. Then, to linear layer(which is a Feed Forward layer). Then, softmax layer translates into a probability distribution which is now human interpretable and the final word is the word correspondent to the highest probability.</li>\n</ol>\n<p>If we stack the blocks to the encoder block, we get the BERT architecture and if the same stacking is done to decoder block, we get the GPT architecture.</p>\n<p>With this knowledge on transformers we can move on to\u00a0BERT.</p>\n<h3>BERT: Bidirectional Encoder Representations from Transformers</h3>\n<p>It is used to solve problems like Machine Translation, Question Answering, Sentimental Analysis, Text Summarization and many more. All of these problems requires the understanding of the language. So, we have two main\u00a0tasks.</p>\n<ol>\n<li>Pre Training BERT to understand the language.</li>\n<li>Fine Tuning BERT to learn specific\u00a0task.</li>\n</ol>\n<p><strong>PRE TRAINING:</strong></p>\n<p>The goal of pre training is to make learn BERT what is language and also what is the context. Basically BERT learns language by training on two unsupervised tasks simultaneously. They\u00a0are</p>\n<p><strong>a) Masked Language Modelling(MLM)</strong></p>\n<p><strong>b) Next Sentence Prediction(NSP)</strong></p>\n<p>For MLM, BERT takes in a sentence and masks some words randomly. The goal is to find these word tokens which are masked(like fill in the blanks). It helps BERT understand bidirectional context within a sentence.</p>\n<p>Example: The fox [MASK 1] over the [MASK 2]\u00a0dog.</p>\n<p>MASK 1\u200a\u2014\u200aJumped.</p>\n<p>MASK 2\u200a\u2014\u200alazy.</p>\n<p>In case of Next Sentence Prediction(NSP), BERT takes in the sentences and determines which sentence comes after the other. Because of this, BERT understands the context across different sentences themselves.</p>\n<p>Example:</p>\n<p>Sent 1\u200a\u2014\u200aCristiano Ronaldo is a goal\u00a0machine.</p>\n<p>Sent 2\u200a\u2014\u200aHe is the GOAT(Greatest Of All\u00a0Time).</p>\n<p>Here Sent 1 followed by Sent\u00a02.</p>\n<p>Now, using both(MLM, NSP) BERT gets a good understanding of the language. Generally these two are trained simultaneously in practise.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/740/0*bDk_-2EUFpDrkRvR.png\"><figcaption>Fig 2</figcaption></figure><p>The input is a set of 2 sentences with some of the words being masked. Each token is a word and then we convert each of these tokens into <strong><em>embeddings</em></strong> using pretrained embeddings. This provides a good starting point for BERT to work\u00a0with.</p>\n<p>During Pre Training, in the input side, how are we going to generate the embeddings from the word token\u00a0inputs?</p>\n<p>Think on it Aliens\u2026\u2026\u2026\u2026\u2026\u2026</p>\n<p>Here comes the answer for that, These initial embeddings constructed from a combination of three kinds of embeddings.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*1sFmGbb-3t1WnB-A.png\"><figcaption>Fig 3: Contextualized word embeddings</figcaption></figure><p><strong>Token Embeddings:</strong> These are the pre trained embeddings. (Word Pieces Embeddings of vocabulary size\u00a030K).</p>\n<p><strong>Sentence Embeddings:</strong> It is nothing but the sentence number that is encoded into a\u00a0vector.</p>\n<p><strong>Position Embeddings:</strong> It is the position of a word within the sentence that is encoded into a\u00a0vector.</p>\n<p>Adding these three vectors together we get an embedding vector that we use as an input to BERT. The sentence and position embeddings are required for temporal ordering since all these vectors are fed in simultaneously into BERT and language modelling needs this ordering preserved.</p>\n<p>Now the output side, the output is a binary value(C) and a bunch of word vectors. \u2018C\u2019 is the binary output for the NSP. So, if C is 1 Sentence B follows Sentence A and C is 0 if Sentence A follows Sentence B. Each of the T\u2019s in the image are the word vectors that corresponds to the outputs for the masked language model problem.But with training, we need to minimize the loss. Here two key details to note. All of the word vectors(T\u2019s) have the same size and all of them are generated simultaneously.</p>\n<p>We need to take each word vector and pass it to a fully connected layered output with the same number of neurons equals to the number of tokens in vocabulary(i.e 30K). We apply a softmax activation. This way we can convert a word vector to a distribution and the actual label of this distribution would be an one hot encoded vector for the actual word. So, we compare these two distributions and train the network using cross entropy loss. Here the loss will be calculated for [MASK]ed words only. This is done to keep more focus on predicting the masked values so that it gets them correct and it tries to improve the\u00a0context.</p>\n<p><strong>FINE TUNING:</strong></p>\n<p>How to use language for specific tasks. For example Question Answering task.</p>\n<p>All we need to do is to replace the fully connected output layers of the network with a fresh set of output layers that can basically give the answer to the question. Then we can perform supervised training using a question answering data set. It won\u2019t take long since it is only the output parameters that are learnt from the scratch. The rest of the model parameters are just slightly fine tuned. As a result, training time is\u00a0fast.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/722/0*Pnw03rxPQyLfXG7E.png\"><figcaption>Fig:4 Illustrations of Fine-tuning BERT on Different Tasks</figcaption></figure><p>In the case of Question Answering(Refer to Fig 4), we would train the model by modifying the inputs and the output layer. We pass in the question followed by a passage containing the answer as input. And the output would be start and end words that encapsulate the answer. Assuming that the answer is within the same span of\u00a0text.</p>\n<p>Performance is dependent on how big we want BERT to be. BERT Large Model which have 340 Million parameters can achieve a way higher accuracy than the BERT base model which has only 110 Million parameters.</p>\n<p>I would strongly recommend the Aliens to visit <a href=\"https://huggingface.co/\">HUGGING FACE</a>: State-of-the-Art Natural Language Processing.</p>\n<p>In <strong>Part 2</strong> of this, we will be going to find the sentiment of the champions league finale(FC Bayern Munich vs PSG). For that, we need to do the\u00a0below:</p>\n<ol>\n<li>Scrape the data from\u00a0twitter.</li>\n<li>Text Preprocessing.</li>\n<li>Applying the BERT model on\u00a0it.</li>\n<li>Getting the polarity score and predicting the sentiments.</li>\n</ol>\n<p>Happy Learning\u2026\u2026.</p>\n<p>Thanks for reading the article! If you like my article do \ud83d\udc4f this article. If you want to connect with me in Linkedin, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c030ca9b33b6\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["deep-learning","nlp","bert","transformers","sentimental-analysis"]},{"title":"OCR\u200a\u2014\u200aOptical Character Recognition","pubDate":"2021-07-10 10:45:22","link":"https://medium.com/@karteekmenda93/ocr-optical-character-recognition-f4317300fa8f?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/f4317300fa8f","author":"Karteek Menda","thumbnail":"","description":"\n<h3>OCR\u200a\u2014\u200aOptical Character Recognition</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OdICm0hw9NkL88bvLq7FrA.jpeg\"><figcaption>@source: Wikipedia</figcaption></figure><p>The technology to convert images of typed, handwritten or printed text into machine-encoded text</p>\n<p>Hello Aliens.</p>\n<p>The mission of artificial intelligence(AI) is to assist humans in processing large amounts of data and to automate the routine tasks. This powerful data can put a business strategy on the right track. But before that someone needs to collect data from multiple sources. Managing documents involves many repetitive tasks and requires much of human effort. So, AI can automate this flow, reduce the processing time and thus save resources.</p>\n<h3>OCR</h3>\n<p>Optical Character Recognition(OCR) is the technology for automatic text recognition. We can translate printed, handwritten and scanned documents into a machine readable format. This technology relieves employees of manual entry of data, and reduce human\u00a0errors.</p>\n<p>Information in documents is usually a combination of natural language and semi-structured data in forms of tables, diagrams, symbols etc\u2026 A human can read and understand text regardless of its structure and the way it is represented. And due to natural language processing (NLP), computers can interact with written (as well as spoken) forms of human language.</p>\n<p>In document recognition, NLP, coupled with OCR, finds applications for data retrieval, information extraction. We can extract everything of value for a company(information about consumers, payment data from invoices) from top to bottom in a document. It saves lot of time when you need to convert a lot of printed file or hard copy printed text to word document. Manually typing will take much time which could be efficiently reduced by\u00a0such.</p>\n<p>OCR is a set of very different, computing intense processes where a lot of mathematics, statistics and linguistic is involved.</p>\n<p>lets take an example and look at how powerful this\u00a0is.</p>\n<p>Here, what we will be doing is, we will take an image which is of Hindi text. And we try to extract the text from the\u00a0image.</p>\n<p>The image I have chosen\u00a0is</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/460/0*kUno6wzudWiPER2A.png\"><figcaption>Source: omniglot.com</figcaption></figure><p>So, for this we need to install the below packages.</p>\n<pre>!pip install easyocr<br>!pip install googletrans<br>!pip install gTTS</pre>\n<p>Then we can create objects for easyocr and translator. And follow the below\u00a0code.</p>\n<pre>reader = easyocr.Reader(['hi'])</pre>\n<pre>translator = Translator()</pre>\n<p>We can also check the bounding boxes by just introducing a function which is as follows and then we can tweak the parameters to get the\u00a0best.</p>\n<pre>bounds = reader.readtext('download.png', add_margin=0.60, width_ths=0.50, link_threshold=0.60, decoder='beamsearch', blocklist='=-')</pre>\n<pre>def draw_boxes(image, bounds, color = 'yellow', width=1):<br>    draw = ImageDraw.Draw(image)<br>    for bound in bounds:<br>      p0, p1, p2, p3 = bound[0]<br>      draw.line([*p0, *p1, *p2, *p3, *p0],fill=color,width = width)<br>    return image<br>draw_boxes(im, bounds)</pre>\n<p>So, We can tweak the parameters like add_margin, width_ths, link_threshold, decoder, blocklist etc\u2026 We have various other parameters which can be dealt with in order to get the job done efficiently. So, I would strongly recommend aliens to visit the github of <a href=\"https://github.com/JaidedAI/EasyOCR\">JaidedAI</a> to get further\u00a0details.</p>\n<p>Also, we can use google translator to translate the text which is obtained from the image to our understandable language. Also, I used google text to speech converter where the text is been converted to the language we specify over there. So, this can serve multipurpose.</p>\n<pre>\u0938\u092d\u0940 \u092e\u0928\u0941\u0937\u094d\u092f \u0915\u094b \u0917\u094c\u0930\u0935 \u0914\u0930 \u0905\u0927\u093f\u0915\u093e\u0930\u094b\u0902 \u0915\u0947  \u092e\u093e\u092e\u0932\u0947 \u092e\u0947\u0902 \u091c\u0928\u094d\u092e\u091c\u093e\u0924 \u0938\u094d\u0935\u0924\u0928\u094d\u0924\u0924\u093e \u0914\u0930 \u0938\u092e\u093e\u0928\u0924\u093e \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0939\u0948 \u0909\u0928\u094d\u0939\u0947\u0902\u092c\u0941\u0926\u094d\u0927\u093f \u0914\u0930 \u0905\u0928\u094d\u0924\u0930\u093e\u0924\u094d\u092e\u093e \u0915\u0940 \u0926\u0947\u0928 \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0939\u0948 \u0914\u0930 \u092a\u0930\u0938\u094d\u092a\u0930 \u0909\u0928\u094d\u0939\u0947\u0902 \u092d\u093e\u0908\u091a\u093e\u0930\u0947 \u0935\u0948 \u092a\u0947 \u092c\u0930\u094d\u0924\u093e\u0935  \u092d\u093e\u0935 \u0930 \u0915\u0930\u0928\u093e \u091a\u093e\u0939\u093f\u090f\u0964</pre>\n<p>The output we obtained is almost okay but a small tweaking of the parameters needs to be done which is been intentionally left to the aliens to sort it down. Lets have a look at what the translation for this \u201cHindi\u201d text will be in \u201cEnglish\u201d.</p>\n<pre>All human beings have inherent freedom and equality in terms of pride and rights, they have the wisdom and wisdom of the soul and they should treat each other with brotherhood.</pre>\n<p>Also, we have also made the translation available in mp3 format, so that it can serve someone who are visual impaired.</p>\n<p>Besides this, we have many other open source OCR libraries. Out of which some are <a href=\"https://github.com/tesseract-ocr/\">Tesseract OCR</a>, CuneiForm OCR, Python pyocr, Yunmai OCR technology, ABBYY.</p>\n<p>Google Tesseract OCR is super cool in handling these type of tasks. I would also recommend the aliens to just go through this. This was beautifully explained over\u00a0<a href=\"https://blog.anirudhmergu.com/code/ocr-python-tesseract-ocr/?utm_campaign=quora_referral&amp;utm_medium=textlink&amp;utm_source=quora\">here</a>.</p>\n<p>I will be coming up with \u201cSentimental Classification using BERT\u201d in my upcoming\u00a0article.</p>\n<p>Please do follow me on Medium so that you can receive the\u00a0updates.</p>\n<p>Follow me on Linkedin: <a href=\"http://www.linkedin.com/in/karteek-menda\">www.linkedin.com/in/karteek-menda</a></p>\n<p>Happy Learning\u2026\u2026.</p>\n<p>Bye Aliens.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f4317300fa8f\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>OCR\u200a\u2014\u200aOptical Character Recognition</h3>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*OdICm0hw9NkL88bvLq7FrA.jpeg\"><figcaption>@source: Wikipedia</figcaption></figure><p>The technology to convert images of typed, handwritten or printed text into machine-encoded text</p>\n<p>Hello Aliens.</p>\n<p>The mission of artificial intelligence(AI) is to assist humans in processing large amounts of data and to automate the routine tasks. This powerful data can put a business strategy on the right track. But before that someone needs to collect data from multiple sources. Managing documents involves many repetitive tasks and requires much of human effort. So, AI can automate this flow, reduce the processing time and thus save resources.</p>\n<h3>OCR</h3>\n<p>Optical Character Recognition(OCR) is the technology for automatic text recognition. We can translate printed, handwritten and scanned documents into a machine readable format. This technology relieves employees of manual entry of data, and reduce human\u00a0errors.</p>\n<p>Information in documents is usually a combination of natural language and semi-structured data in forms of tables, diagrams, symbols etc\u2026 A human can read and understand text regardless of its structure and the way it is represented. And due to natural language processing (NLP), computers can interact with written (as well as spoken) forms of human language.</p>\n<p>In document recognition, NLP, coupled with OCR, finds applications for data retrieval, information extraction. We can extract everything of value for a company(information about consumers, payment data from invoices) from top to bottom in a document. It saves lot of time when you need to convert a lot of printed file or hard copy printed text to word document. Manually typing will take much time which could be efficiently reduced by\u00a0such.</p>\n<p>OCR is a set of very different, computing intense processes where a lot of mathematics, statistics and linguistic is involved.</p>\n<p>lets take an example and look at how powerful this\u00a0is.</p>\n<p>Here, what we will be doing is, we will take an image which is of Hindi text. And we try to extract the text from the\u00a0image.</p>\n<p>The image I have chosen\u00a0is</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/460/0*kUno6wzudWiPER2A.png\"><figcaption>Source: omniglot.com</figcaption></figure><p>So, for this we need to install the below packages.</p>\n<pre>!pip install easyocr<br>!pip install googletrans<br>!pip install gTTS</pre>\n<p>Then we can create objects for easyocr and translator. And follow the below\u00a0code.</p>\n<pre>reader = easyocr.Reader(['hi'])</pre>\n<pre>translator = Translator()</pre>\n<p>We can also check the bounding boxes by just introducing a function which is as follows and then we can tweak the parameters to get the\u00a0best.</p>\n<pre>bounds = reader.readtext('download.png', add_margin=0.60, width_ths=0.50, link_threshold=0.60, decoder='beamsearch', blocklist='=-')</pre>\n<pre>def draw_boxes(image, bounds, color = 'yellow', width=1):<br>    draw = ImageDraw.Draw(image)<br>    for bound in bounds:<br>      p0, p1, p2, p3 = bound[0]<br>      draw.line([*p0, *p1, *p2, *p3, *p0],fill=color,width = width)<br>    return image<br>draw_boxes(im, bounds)</pre>\n<p>So, We can tweak the parameters like add_margin, width_ths, link_threshold, decoder, blocklist etc\u2026 We have various other parameters which can be dealt with in order to get the job done efficiently. So, I would strongly recommend aliens to visit the github of <a href=\"https://github.com/JaidedAI/EasyOCR\">JaidedAI</a> to get further\u00a0details.</p>\n<p>Also, we can use google translator to translate the text which is obtained from the image to our understandable language. Also, I used google text to speech converter where the text is been converted to the language we specify over there. So, this can serve multipurpose.</p>\n<pre>\u0938\u092d\u0940 \u092e\u0928\u0941\u0937\u094d\u092f \u0915\u094b \u0917\u094c\u0930\u0935 \u0914\u0930 \u0905\u0927\u093f\u0915\u093e\u0930\u094b\u0902 \u0915\u0947  \u092e\u093e\u092e\u0932\u0947 \u092e\u0947\u0902 \u091c\u0928\u094d\u092e\u091c\u093e\u0924 \u0938\u094d\u0935\u0924\u0928\u094d\u0924\u0924\u093e \u0914\u0930 \u0938\u092e\u093e\u0928\u0924\u093e \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0939\u0948 \u0909\u0928\u094d\u0939\u0947\u0902\u092c\u0941\u0926\u094d\u0927\u093f \u0914\u0930 \u0905\u0928\u094d\u0924\u0930\u093e\u0924\u094d\u092e\u093e \u0915\u0940 \u0926\u0947\u0928 \u092a\u094d\u0930\u093e\u092a\u094d\u0924 \u0939\u0948 \u0914\u0930 \u092a\u0930\u0938\u094d\u092a\u0930 \u0909\u0928\u094d\u0939\u0947\u0902 \u092d\u093e\u0908\u091a\u093e\u0930\u0947 \u0935\u0948 \u092a\u0947 \u092c\u0930\u094d\u0924\u093e\u0935  \u092d\u093e\u0935 \u0930 \u0915\u0930\u0928\u093e \u091a\u093e\u0939\u093f\u090f\u0964</pre>\n<p>The output we obtained is almost okay but a small tweaking of the parameters needs to be done which is been intentionally left to the aliens to sort it down. Lets have a look at what the translation for this \u201cHindi\u201d text will be in \u201cEnglish\u201d.</p>\n<pre>All human beings have inherent freedom and equality in terms of pride and rights, they have the wisdom and wisdom of the soul and they should treat each other with brotherhood.</pre>\n<p>Also, we have also made the translation available in mp3 format, so that it can serve someone who are visual impaired.</p>\n<p>Besides this, we have many other open source OCR libraries. Out of which some are <a href=\"https://github.com/tesseract-ocr/\">Tesseract OCR</a>, CuneiForm OCR, Python pyocr, Yunmai OCR technology, ABBYY.</p>\n<p>Google Tesseract OCR is super cool in handling these type of tasks. I would also recommend the aliens to just go through this. This was beautifully explained over\u00a0<a href=\"https://blog.anirudhmergu.com/code/ocr-python-tesseract-ocr/?utm_campaign=quora_referral&amp;utm_medium=textlink&amp;utm_source=quora\">here</a>.</p>\n<p>I will be coming up with \u201cSentimental Classification using BERT\u201d in my upcoming\u00a0article.</p>\n<p>Please do follow me on Medium so that you can receive the\u00a0updates.</p>\n<p>Follow me on Linkedin: <a href=\"http://www.linkedin.com/in/karteek-menda\">www.linkedin.com/in/karteek-menda</a></p>\n<p>Happy Learning\u2026\u2026.</p>\n<p>Bye Aliens.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f4317300fa8f\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["nlp","easyocr","deep-learning","ocr"]},{"title":"A Beginner\u2019s Guide to NLP","pubDate":"2021-07-10 10:35:51","link":"https://medium.com/@karteekmenda93/a-beginners-guide-to-nlp-da13fae5e885?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/da13fae5e885","author":"Karteek Menda","thumbnail":"","description":"\n<p>This article is dedicated to Late <a href=\"https://www.britannica.com/biography/Alan-Turing\">Alan\u00a0Turing</a>.</p>\n<p>Hello Aliens</p>\n<h3>Natural Language Processing(NLP)</h3>\n<p>Natural Language Processing, usually called as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages. NLP plays a critical role in supporting machine-human interactions.</p>\n<p>In this article, I will let you know some of the NLP tasks which were performed and later we will deploy on to the web to make it a complete\u00a0package.</p>\n<p>The tasks are mentioned below.</p>\n<ol>\n<li>Analyzing the text and getting the tokens and lemma of the\u00a0text.</li>\n<li>Also getting the NER(Named Entity Recognition) from the text\u00a0entered.</li>\n<li>Sentimental Analysis.</li>\n<li>Text Summarization (Extract Summarization)</li>\n<li>Machine Translation.</li>\n</ol>\n<p>I will throw some light on each and every task mentioned above as we proceed\u00a0further.</p>\n<h3>1. Tokens and\u00a0Lemma</h3>\n<p>A token is the smallest part of a corpus. And tokenization is the task of chopping it up into pieces, called\u00a0tokens.</p>\n<p>For example:</p>\n<p>Input: NLP and Machine learning go hand in\u00a0hand.</p>\n<p>After Tokenization, the output is nothing but each of the word present in this sentence. NLP is one token Machine is another token and this list goes on like\u00a0this.</p>\n<p>Lemma is like getting to a root of that given word. Lemma uses wordnet corpus. It can be used when we want more human understandable words, as the output of lemmatization is a proper word. It will be more clear with an\u00a0example.</p>\n<p>Lets take three words \u201cgoing\u201d, \u201cgoes\u201d, \u201cgone\u201d. The lemma is nothing but getting the root word which is\u00a0\u201cgo\u201d.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/60/0*08EEzkuc8YjkPO2D\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/393/0*XHgyVKoUQk7pcYaR.png\"><figcaption>Tokens and Lemma of the\u00a0text.</figcaption></figure><h3>2. NER(Named Entity Recognition)</h3>\n<p>In any text document, there will be particular terms that represent specific entities that are more informative and have a unique context. These entities are known as named entities\u00a0, which more specifically are real-world objects like people, places, organizations, and so on, which are often denoted by proper\u00a0names.</p>\n<p>Named entity recognition (NER)\u00a0, also known as entity chunking/extraction, is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/60/0*wML01VfLDHvMZNcA\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/477/0*WBMG1VpTAY0W40nG.png\"><figcaption>Named Entity Recognition</figcaption></figure><h3>3. Sentimental Analysis.</h3>\n<p>Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis tools allow businesses to identify customer sentiment toward products, brands or services in online\u00a0feedback</p>\n<p>A sentiment analysis system for text analysis combines natural language processing (<a href=\"https://www.lexalytics.com/lexablog/what-is-natural-language-processing\">NLP</a>) and machine learning techniques to assign weighted sentiment scores to the entities, topics, themes and categories within a sentence or\u00a0phrase.</p>\n<p>For example: Lets take a movie review \u201c this movie was the worst of times\u201d. Of course this is a negative sentiment. We as humans can say that but what about Machines. So, we have nice package named TextBlob which could tell us the sentiment of this text, and in the background we are using the default NaiveBayes Analyzer.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*ejwe9g4HFKizLch-.png\"></figure><p>The review is a negative sentiment and probability of this review being negative is almost 80.5%. Great\u00a0job.</p>\n<h3>4. Text Summarization</h3>\n<p>Text summarization refers to the technique of shortening long pieces of text. The intention is to create a fluent summary while preserving key information content and overall meaning. Applying text summarization reduces reading time, accelerates the process of researching for information, and increases the amount of information that can fit in an\u00a0area.</p>\n<p>Summaries reduce the reading time. Automatic summarization improves the effectiveness of the indexing and are less biased than the human summarizers.</p>\n<p>There are broadly two different approaches that are used for text summarization:</p>\n<ol>\n<li>Extractive Summarization</li>\n<li>Abstractive Summarization</li>\n</ol>\n<p>However I would go with extractive summarization for this. I strongly recommend the Aliens to go through this Text Summarization and dig deep as there are number of algorithms which are available to summarize the text. I will be posting a seperate article on Text summarization in a short\u00a0period.</p>\n<p>I have used almost 4 algorithms to summarize the\u00a0text.</p>\n<h3>Gensim.</h3>\n<ul>\n<li>This module provides functions for summarizing texts. Summarizing is based on ranks of text sentences using a variation of the TextRank algorithm.</li>\n<li>This module automatically summarizes the given text, by extracting one or more important sentences from the text. In a similar way, it can also extract keywords.</li>\n</ul>\n<h3>Lex rank.</h3>\n<ul>\n<li>Unsupervised approach to text summarization based on graph-based centrality scoring of sentences.</li>\n<li>The main idea is that sentences \u201crecommend\u201d other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of greater importance.</li>\n</ul>\n<h3>Luhn.</h3>\n<ul><li>Based on frequency of most important words.</li></ul>\n<h3>LSA.</h3>\n<ul><li>based on term frequency techniques with singular value decomposition to summarize texts.</li></ul>\n<h3>5. Machine Translation.</h3>\n<p>Machine translation (MT) is an automatic translation from one language to another. Machine translation refers to fully automated software that can translate source content into target languages. Humans may use MT to help them render text and speech into another language.</p>\n<p>Over here I used the package TextBlob for the translation. The input can be any language text. The output will be the specified language which the end user\u00a0wants.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/320/0*-uPxSq2ztMW6udUZ.png\"><figcaption>English to\u00a0Hindi</figcaption></figure><p>The input sentence is an English text and the output we want it to be translated to Hindi. And, we got\u00a0it.</p>\n<p>Also, the input can be of any language. Lets go with one more language\u00a0Arabic.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/287/0*DJpFFzuwIUZRam39.png\"><figcaption>Arabic to\u00a0Hindi</figcaption></figure><p>Lets cross check\u00a0once.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/279/0*9WuNAuSvVG2laaoH.png\"><figcaption>Arabic to\u00a0English</figcaption></figure><p>Now lets move on to the deployment.</p>\n<p>So, same we will be importing all the packages and creating functions and calling them whenever the user selects the particular field. You can find the entire code for this\u00a0<a href=\"https://www.kaggle.com/karteek93/nlp-task-deployment\">here</a>.</p>\n<p>So, after the deployment is done, we can see a nice web app which is ready to serve the business.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*vchbUPhWss8AchOJ.png\"><figcaption>Web App</figcaption></figure><p>A simple and beautiful web app is created. Now you can use Heroku for cloud deployment.</p>\n<p>I will try to take away your confusion in \u201cConfusion Matrix\u201d in my upcoming\u00a0article.</p>\n<p>Please do follow me on Medium so that you can receive the\u00a0updates.</p>\n<p>Happy Learning\u2026\u2026.</p>\n<p>Follow me on Linkedin: <a href=\"http://www.linkedin.com/in/karteek-menda\">www.linkedin.com/in/karteek-menda</a></p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=da13fae5e885\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>This article is dedicated to Late <a href=\"https://www.britannica.com/biography/Alan-Turing\">Alan\u00a0Turing</a>.</p>\n<p>Hello Aliens</p>\n<h3>Natural Language Processing(NLP)</h3>\n<p>Natural Language Processing, usually called as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning from human languages. NLP plays a critical role in supporting machine-human interactions.</p>\n<p>In this article, I will let you know some of the NLP tasks which were performed and later we will deploy on to the web to make it a complete\u00a0package.</p>\n<p>The tasks are mentioned below.</p>\n<ol>\n<li>Analyzing the text and getting the tokens and lemma of the\u00a0text.</li>\n<li>Also getting the NER(Named Entity Recognition) from the text\u00a0entered.</li>\n<li>Sentimental Analysis.</li>\n<li>Text Summarization (Extract Summarization)</li>\n<li>Machine Translation.</li>\n</ol>\n<p>I will throw some light on each and every task mentioned above as we proceed\u00a0further.</p>\n<h3>1. Tokens and\u00a0Lemma</h3>\n<p>A token is the smallest part of a corpus. And tokenization is the task of chopping it up into pieces, called\u00a0tokens.</p>\n<p>For example:</p>\n<p>Input: NLP and Machine learning go hand in\u00a0hand.</p>\n<p>After Tokenization, the output is nothing but each of the word present in this sentence. NLP is one token Machine is another token and this list goes on like\u00a0this.</p>\n<p>Lemma is like getting to a root of that given word. Lemma uses wordnet corpus. It can be used when we want more human understandable words, as the output of lemmatization is a proper word. It will be more clear with an\u00a0example.</p>\n<p>Lets take three words \u201cgoing\u201d, \u201cgoes\u201d, \u201cgone\u201d. The lemma is nothing but getting the root word which is\u00a0\u201cgo\u201d.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/60/0*08EEzkuc8YjkPO2D\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/393/0*XHgyVKoUQk7pcYaR.png\"><figcaption>Tokens and Lemma of the\u00a0text.</figcaption></figure><h3>2. NER(Named Entity Recognition)</h3>\n<p>In any text document, there will be particular terms that represent specific entities that are more informative and have a unique context. These entities are known as named entities\u00a0, which more specifically are real-world objects like people, places, organizations, and so on, which are often denoted by proper\u00a0names.</p>\n<p>Named entity recognition (NER)\u00a0, also known as entity chunking/extraction, is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/60/0*wML01VfLDHvMZNcA\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/477/0*WBMG1VpTAY0W40nG.png\"><figcaption>Named Entity Recognition</figcaption></figure><h3>3. Sentimental Analysis.</h3>\n<p>Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis tools allow businesses to identify customer sentiment toward products, brands or services in online\u00a0feedback</p>\n<p>A sentiment analysis system for text analysis combines natural language processing (<a href=\"https://www.lexalytics.com/lexablog/what-is-natural-language-processing\">NLP</a>) and machine learning techniques to assign weighted sentiment scores to the entities, topics, themes and categories within a sentence or\u00a0phrase.</p>\n<p>For example: Lets take a movie review \u201c this movie was the worst of times\u201d. Of course this is a negative sentiment. We as humans can say that but what about Machines. So, we have nice package named TextBlob which could tell us the sentiment of this text, and in the background we are using the default NaiveBayes Analyzer.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*ejwe9g4HFKizLch-.png\"></figure><p>The review is a negative sentiment and probability of this review being negative is almost 80.5%. Great\u00a0job.</p>\n<h3>4. Text Summarization</h3>\n<p>Text summarization refers to the technique of shortening long pieces of text. The intention is to create a fluent summary while preserving key information content and overall meaning. Applying text summarization reduces reading time, accelerates the process of researching for information, and increases the amount of information that can fit in an\u00a0area.</p>\n<p>Summaries reduce the reading time. Automatic summarization improves the effectiveness of the indexing and are less biased than the human summarizers.</p>\n<p>There are broadly two different approaches that are used for text summarization:</p>\n<ol>\n<li>Extractive Summarization</li>\n<li>Abstractive Summarization</li>\n</ol>\n<p>However I would go with extractive summarization for this. I strongly recommend the Aliens to go through this Text Summarization and dig deep as there are number of algorithms which are available to summarize the text. I will be posting a seperate article on Text summarization in a short\u00a0period.</p>\n<p>I have used almost 4 algorithms to summarize the\u00a0text.</p>\n<h3>Gensim.</h3>\n<ul>\n<li>This module provides functions for summarizing texts. Summarizing is based on ranks of text sentences using a variation of the TextRank algorithm.</li>\n<li>This module automatically summarizes the given text, by extracting one or more important sentences from the text. In a similar way, it can also extract keywords.</li>\n</ul>\n<h3>Lex rank.</h3>\n<ul>\n<li>Unsupervised approach to text summarization based on graph-based centrality scoring of sentences.</li>\n<li>The main idea is that sentences \u201crecommend\u201d other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of greater importance.</li>\n</ul>\n<h3>Luhn.</h3>\n<ul><li>Based on frequency of most important words.</li></ul>\n<h3>LSA.</h3>\n<ul><li>based on term frequency techniques with singular value decomposition to summarize texts.</li></ul>\n<h3>5. Machine Translation.</h3>\n<p>Machine translation (MT) is an automatic translation from one language to another. Machine translation refers to fully automated software that can translate source content into target languages. Humans may use MT to help them render text and speech into another language.</p>\n<p>Over here I used the package TextBlob for the translation. The input can be any language text. The output will be the specified language which the end user\u00a0wants.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/320/0*-uPxSq2ztMW6udUZ.png\"><figcaption>English to\u00a0Hindi</figcaption></figure><p>The input sentence is an English text and the output we want it to be translated to Hindi. And, we got\u00a0it.</p>\n<p>Also, the input can be of any language. Lets go with one more language\u00a0Arabic.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/287/0*DJpFFzuwIUZRam39.png\"><figcaption>Arabic to\u00a0Hindi</figcaption></figure><p>Lets cross check\u00a0once.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/279/0*9WuNAuSvVG2laaoH.png\"><figcaption>Arabic to\u00a0English</figcaption></figure><p>Now lets move on to the deployment.</p>\n<p>So, same we will be importing all the packages and creating functions and calling them whenever the user selects the particular field. You can find the entire code for this\u00a0<a href=\"https://www.kaggle.com/karteek93/nlp-task-deployment\">here</a>.</p>\n<p>So, after the deployment is done, we can see a nice web app which is ready to serve the business.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*vchbUPhWss8AchOJ.png\"><figcaption>Web App</figcaption></figure><p>A simple and beautiful web app is created. Now you can use Heroku for cloud deployment.</p>\n<p>I will try to take away your confusion in \u201cConfusion Matrix\u201d in my upcoming\u00a0article.</p>\n<p>Please do follow me on Medium so that you can receive the\u00a0updates.</p>\n<p>Happy Learning\u2026\u2026.</p>\n<p>Follow me on Linkedin: <a href=\"http://www.linkedin.com/in/karteek-menda\">www.linkedin.com/in/karteek-menda</a></p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=da13fae5e885\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["nlp","named-entity-recognition","sentiment-analysis","text-summarization","machine-translation"]},{"title":"Deployment of Machine Learning Application Made Easy","pubDate":"2021-07-10 10:30:43","link":"https://medium.com/@karteekmenda93/deployment-of-machine-learning-application-made-easy-f61ba593c318?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/f61ba593c318","author":"Karteek Menda","thumbnail":"","description":"\n<p>ML application development is really fun and these apps help data scientists and machine learning engineers to solve business problems.</p>\n<p>Hello Aliens. In this, we will be going through the complete life cycle of a ML project. However, the main focus is on the front end which can be done very easily using <a href=\"https://docs.streamlit.io/\">Streamlit</a>. This is one of the coolest platforms available which makes the life of a ML Engineer, data Scientist\u2019s life easy. So, Lets begin the\u00a0show.</p>\n<p>The problem statement is Bank Churn attrition. The data set is from Kaggle and is available <a href=\"https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers\">here</a>. This has become very crucial now a days for the business. Let me throw some light on the what is the problem statement and why it needs to put to a\u00a0check.</p>\n<p>Customer attrition is the concern of most business that are involved in low switching cost markets. Banking industry can be one of the top most sufferers with a higher churn rate. the capability to predict that a customer is at high risk of churning, while there is still time to prevent this is a huge additional potential revenue source for every business. Some studies confirmed that acquiring new customers can cost five times more than satisfying and retaining existing customers. As a matter of fact, there are a lot of benefits that encourage the tracking of the customer churn rate as marketing costs to acquire new customers are high. Therefore, it is important to retain customers so that the initial investment is not\u00a0wasted.</p>\n<p>Basically this is of two\u00a0steps.</p>\n<ol>\n<li>Building ML models and saving the\u00a0model.</li>\n<li>using the saved model and creating a web application.</li>\n</ol>\n<p>The ML models have been built and after hyper parameter tuning\u00a0, I can be able to get a final model of XGBoost which have pretty impressive metrics. The code for this is been available <a href=\"https://www.kaggle.com/karteek93/bank-churn-attrition\">here</a>. So, please visit the link to get the complete idea of what is been done and how the hyper parameter tuning is done\u00a0etc\u2026\u2026</p>\n<p>The metrics for the best model which is XGBoost(in my case) is as\u00a0follows.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*dfsw6je2IPzfIdme.png\"></figure><p>Now let\u2019s analyse the classification report.</p>\n<p>\u2022 True positives (TP): These are cases in which we predicted yes (they churn), and they do churn as well.\u200a\u2014\u200a(1473)<br>\u2022 True negatives (TN): We predicted no, and they don\u2019t churn.\u200a\u2014\u200a(1550)<br>\u2022 False positives (FP): We predicted yes (they churn), but they don\u2019t actually churn.\u200a\u2014\u200a(132)<br>\u2022 False negatives (FN): We predicted no, but they actually churn.\u200a\u2014\u200a(31).</p>\n<p>In these types of scenarios, we need to have a close eye on False negatives, as that would be the most important figure which needs special attention. The metric for this model is showing way less false Negatives when compared with the other models. Also, we have good precision, recall\u00a0values.</p>\n<p>Now, we have finalised the best model and that needs to be pickled and save it for our further\u00a0usage.</p>\n<h3>Creating UI</h3>\n<h3>Now we use Streamlit to create\u00a0UI.</h3>\n<p>Importing the packages.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/243/0*-zdOOrO2HkTubsTf.png\"></figure><p>Use the pickle files which were created earlier and load them over\u00a0here.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/645/0*MT9XSkb36tNF2_hL.png\"></figure><p>Create variable selection with this simple\u00a0code.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*AGrxVDSdk8RH3evP.png\"></figure><p>Create a function which can give you the corresponding probability of a customer being churned and here we use the two best algorithms in our case(XGBoost, Random\u00a0Forest).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*nVQUmxdwnBgzhfS1.png\"></figure><p>Create a button named \u201cpredict\u201d. So, once the button is fired after giving all the inputs, the function should be called and based on the algorithm(in our case, we have 2), the corresponding probability needs to be obtained and if the probability obtained is greater than 0.5, the customer is going to churn and vice\u00a0versa.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*lpQpWuGTgWdZ_9np.png\"></figure><p>That\u2019s it.</p>\n<p>Save this in app.py or else any name. And then open the terminal and type \u201cstreamlit run app.py\u201d. Then you will be prompted\u00a0this.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/536/0*7MekIFuOsRhNy9G8.png\"></figure><p>Open your browser. And you can see the app which is ready to serve the business.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*DCxrfOZnzfVsO9zK.png\"></figure><p>A simple and beautiful web app is created in less than 60 minutes. Now you can use Heroku for cloud deployment.</p>\n<p>I will be posting deep learning applications and NLP tasks in my upcoming articles.</p>\n<p>Happy Learning\u2026\u2026\u2026\u2026</p>\n<p>Thanks for reading the article! If you like my article do \ud83d\udc4f this article. If you want to connect with me in Linkedin, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f61ba593c318\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>ML application development is really fun and these apps help data scientists and machine learning engineers to solve business problems.</p>\n<p>Hello Aliens. In this, we will be going through the complete life cycle of a ML project. However, the main focus is on the front end which can be done very easily using <a href=\"https://docs.streamlit.io/\">Streamlit</a>. This is one of the coolest platforms available which makes the life of a ML Engineer, data Scientist\u2019s life easy. So, Lets begin the\u00a0show.</p>\n<p>The problem statement is Bank Churn attrition. The data set is from Kaggle and is available <a href=\"https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers\">here</a>. This has become very crucial now a days for the business. Let me throw some light on the what is the problem statement and why it needs to put to a\u00a0check.</p>\n<p>Customer attrition is the concern of most business that are involved in low switching cost markets. Banking industry can be one of the top most sufferers with a higher churn rate. the capability to predict that a customer is at high risk of churning, while there is still time to prevent this is a huge additional potential revenue source for every business. Some studies confirmed that acquiring new customers can cost five times more than satisfying and retaining existing customers. As a matter of fact, there are a lot of benefits that encourage the tracking of the customer churn rate as marketing costs to acquire new customers are high. Therefore, it is important to retain customers so that the initial investment is not\u00a0wasted.</p>\n<p>Basically this is of two\u00a0steps.</p>\n<ol>\n<li>Building ML models and saving the\u00a0model.</li>\n<li>using the saved model and creating a web application.</li>\n</ol>\n<p>The ML models have been built and after hyper parameter tuning\u00a0, I can be able to get a final model of XGBoost which have pretty impressive metrics. The code for this is been available <a href=\"https://www.kaggle.com/karteek93/bank-churn-attrition\">here</a>. So, please visit the link to get the complete idea of what is been done and how the hyper parameter tuning is done\u00a0etc\u2026\u2026</p>\n<p>The metrics for the best model which is XGBoost(in my case) is as\u00a0follows.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/788/0*dfsw6je2IPzfIdme.png\"></figure><p>Now let\u2019s analyse the classification report.</p>\n<p>\u2022 True positives (TP): These are cases in which we predicted yes (they churn), and they do churn as well.\u200a\u2014\u200a(1473)<br>\u2022 True negatives (TN): We predicted no, and they don\u2019t churn.\u200a\u2014\u200a(1550)<br>\u2022 False positives (FP): We predicted yes (they churn), but they don\u2019t actually churn.\u200a\u2014\u200a(132)<br>\u2022 False negatives (FN): We predicted no, but they actually churn.\u200a\u2014\u200a(31).</p>\n<p>In these types of scenarios, we need to have a close eye on False negatives, as that would be the most important figure which needs special attention. The metric for this model is showing way less false Negatives when compared with the other models. Also, we have good precision, recall\u00a0values.</p>\n<p>Now, we have finalised the best model and that needs to be pickled and save it for our further\u00a0usage.</p>\n<h3>Creating UI</h3>\n<h3>Now we use Streamlit to create\u00a0UI.</h3>\n<p>Importing the packages.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/243/0*-zdOOrO2HkTubsTf.png\"></figure><p>Use the pickle files which were created earlier and load them over\u00a0here.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/645/0*MT9XSkb36tNF2_hL.png\"></figure><p>Create variable selection with this simple\u00a0code.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*AGrxVDSdk8RH3evP.png\"></figure><p>Create a function which can give you the corresponding probability of a customer being churned and here we use the two best algorithms in our case(XGBoost, Random\u00a0Forest).</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*nVQUmxdwnBgzhfS1.png\"></figure><p>Create a button named \u201cpredict\u201d. So, once the button is fired after giving all the inputs, the function should be called and based on the algorithm(in our case, we have 2), the corresponding probability needs to be obtained and if the probability obtained is greater than 0.5, the customer is going to churn and vice\u00a0versa.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*lpQpWuGTgWdZ_9np.png\"></figure><p>That\u2019s it.</p>\n<p>Save this in app.py or else any name. And then open the terminal and type \u201cstreamlit run app.py\u201d. Then you will be prompted\u00a0this.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/536/0*7MekIFuOsRhNy9G8.png\"></figure><p>Open your browser. And you can see the app which is ready to serve the business.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*DCxrfOZnzfVsO9zK.png\"></figure><p>A simple and beautiful web app is created in less than 60 minutes. Now you can use Heroku for cloud deployment.</p>\n<p>I will be posting deep learning applications and NLP tasks in my upcoming articles.</p>\n<p>Happy Learning\u2026\u2026\u2026\u2026</p>\n<p>Thanks for reading the article! If you like my article do \ud83d\udc4f this article. If you want to connect with me in Linkedin, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f61ba593c318\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["machine-learning","streamlit","ml-model-deployment","churn-prediction"]},{"title":"H2O-Automated ML","pubDate":"2021-07-10 10:26:39","link":"https://medium.com/@karteekmenda93/h2o-automated-ml-a88806206782?source=rss-41e101cb82b5------2","guid":"https://medium.com/p/a88806206782","author":"Karteek Menda","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/835/1*jASYgBSbqpLE_yFCi545CA.jpeg\"><figcaption>@credits: H2O.ai</figcaption></figure><p>Mission to make AI for\u00a0everyone</p>\n<p>Hello Aliens\u2026.</p>\n<p>H2O.ai is the open source leader in AI and machine learning with a mission to democratize AI for everyone. The motive of H2O is to provide a platform which made easy for the non-experts to do experiments with machine learning.</p>\n<p>So, the automation of your Machine Learning algorithms which will reduce the daily task of Data Scientists. This gives us the leverage to skip the selection of baseline models. By using this, we can simply get to an understanding of which model to go with and then by finalizing the model, we can carry out out optimization on it to get better\u00a0results.</p>\n<p>AutoML helps in automatic training and tuning of many models within a user-specified time limit. Some of the key features of H2OAutoML are it can do data pre-processing(encoding), data cleaning(missing value imputation) also provides us nice leaderboard view of the models with various metrics. So, we can pick any model from it and analyze the model output further. It also provides a deployment ready code. It gives in multiple formats like Mojo, Pojo, Binary formats. Out of which Mojo is the recommended format when the model size is huge. It uses GPU\u2019s for XGBoost\u00a0model.</p>\n<h3>Architecture:</h3>\n<p>H2O architecture can be divided into different layers in which the top layer will be different APIs, and the bottom layer will be H2O\u00a0JVM.</p>\n<p>H2O\u2019s core code is written in Java that enables the whole framework for multi-threading. Even though it is written in Java, it provides interfaces for R, Python and few others shown in the architecture.</p>\n<p>In short, we can say that H2O is an open source, in memory, distributed, fast and scalable machine learning and predictive analytics that allow building machine learning models to be an\u00a0ease.</p>\n<p>Now let me walk you through some use case which will give you some clarity on how it\u00a0works.</p>\n<p>For this demonstration, I am taking a imbalanced dataset which is Bank Churn dataset from Kaggle. You can find that\u00a0<a href=\"https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers\">here</a>.</p>\n<p>Lets go through the\u00a0code.</p>\n<p><strong><em>Step-1</em></strong>: To install H2O, you need to have java run time environment as it is developed on java. So, install the java run time environment.</p>\n<pre>!apt-get install default-jre<br>!java -version</pre>\n<p><strong><em>Step-2</em></strong>: Install H2O and import\u00a0it.</p>\n<pre>!pip install h2o<br>import h2o</pre>\n<p><strong><em>Step-3</em></strong>: Initialize the H2O\u00a0cluster.</p>\n<pre>h2o.init()</pre>\n<p>The h2o.init() command is pretty smart and does a lot of work. At first, it looks for any active h2o instance before starting a new one and then starts a new one when instance are not present. Once the instance is initiated, we can see the <strong><em>flow</em></strong> running on <a href=\"http://127.0.0.1:54321/\">http://127.0.0.1:54321</a>.</p>\n<p><strong><em>Step-4</em></strong>: Load the data to H2O\u00a0frame.</p>\n<pre>from h2o.automl import H2OAutoML<br>bank_data = h2o.import_file('Churn.csv')</pre>\n<p>The reason why I have chosen this dataset is it has class imbalance and some decent data preprocessing needs to be done and see how AutoML deals with all\u00a0this.</p>\n<p><strong><em>Step-5</em></strong>: EDA of the\u00a0data.</p>\n<pre>bank_data.describe()<br>bank_data.types</pre>\n<p>Can do EDA\u00a0, but this is not our main focus, so skipping this. The dataset is having 14 columns out of which the target variable is \u201cExited\u201d(which says whether a customer has churned or not) there are some features like \u201cRowNumber\u201d, \u201cCustomerID\u201d, \u201cSurname\u201d which can be removed. There are some features which are categorical(Gender, Geography).</p>\n<p><strong><em>Step-6</em></strong>: Split the data to train, test sets with a split of 80% and 20% respectively.</p>\n<pre>train, test= bank_data.split_frame(ratios = [.8], seed = 1234)</pre>\n<p><strong><em>Step-7</em></strong>: selecting the predictors and the predicted variable.</p>\n<pre>y = 'Exited'<br>x = bank_data.columns<br>x.remove(y)<br>x.remove(\"RowNumber\")<br>x.remove(\"CustomerId\")<br>x.remove(\"Surname\")</pre>\n<p><strong><em>Step-8</em></strong>: Usage of H2OAutoML.</p>\n<pre>aml = H2OAutoML(max_models=20, seed = 10, balance_classes=True, exclude_algos = [\"StackedEnsemble\", \"DeepLearning\"], verbosity = \"info\", nfolds=0)</pre>\n<p>So, here I want the top 20 models, and we have a class imbalance so set it to True would balance the data and I don\u2019t want the algorithms like Stacked Ensemble, Deep Learning. So I can exclude them. And cross validations as 0 to keep it simple, But you can tweak this for better results. By default \u201cnfolds\u201d will be set to 5. Also, we can use some time boxes such that the models will not run for more than that specified time.</p>\n<p><strong><em>Step-9</em></strong>: Call the train function by passing input features and the output column and training data\u00a0frame.</p>\n<pre>aml.train(x=x,y=y,training_frame= train)</pre>\n<p>It will train for 20 different models and each time leader board gets\u00a0updated.</p>\n<p><strong><em>Step-10</em></strong>: Check the leaderboard to see the top performing models.</p>\n<pre>lb = aml.leaderboard<br>lb.head()</pre>\n<p>In this case, GBM tops the list then comes\u00a0XGBoost.</p>\n<p><strong><em>Step-11</em></strong>: Use the \u201cleader\u201d and predict on test\u00a0dataset.</p>\n<pre>prediction = aml.leader.predict(test)</pre>\n<p>And you can see the predictions with probabilities being churned or\u00a0not.</p>\n<p><strong><em>Step-12</em></strong>: Generate a performance report.</p>\n<pre>aml.leader.model_performance(test)</pre>\n<p>This report shows the metrics of the\u00a0model.</p>\n<p><strong><em>Step-13</em></strong>: So out of all the models, lets take a particular model from the leaderboard and analyze it further. Here I want XGBoost to be analyzed further. Lets take this XGBoost\u00a0model.</p>\n<pre>model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])<br>xgb=h2o.get_model([mid for mid in model_ids if \"XGBoost\" in mid][0])</pre>\n<p><strong><em>Step-14</em></strong>: Let see the output which gives us the model details and the\u00a0metrics.</p>\n<pre>xgb</pre>\n<p><strong><em>Step-15</em></strong>: Plot for the variable importance which gives us the list of the most significant variables. The top variables contribute more to the model than the bottom ones and also have high predictive power in classifying churn and no-churn customers.</p>\n<pre>xgb.varimp_plot()</pre>\n<h3>Conclusion:</h3>\n<p>H2O provides an easy-to-use open source platform for applying different ML algorithms on a given dataset. During testing, you can fine tune the parameters to these algorithms. H2O supports AutoML that provides the ranking amongst the several algorithms based on their performance. It can also handle Big Data. This is definitely a boon for Data Scientist to apply the different Machine Learning models on their dataset and pick up the best one to meet their\u00a0needs.</p>\n<p>Happy Learning\u2026\u2026\u2026\u2026</p>\n<p>Thanks for reading the article! If you like my article do \ud83d\udc4f this article. If you want to connect with me in Linkedin, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I will try to explain <strong>H2O Flow</strong> in my upcoming articles.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a88806206782\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/835/1*jASYgBSbqpLE_yFCi545CA.jpeg\"><figcaption>@credits: H2O.ai</figcaption></figure><p>Mission to make AI for\u00a0everyone</p>\n<p>Hello Aliens\u2026.</p>\n<p>H2O.ai is the open source leader in AI and machine learning with a mission to democratize AI for everyone. The motive of H2O is to provide a platform which made easy for the non-experts to do experiments with machine learning.</p>\n<p>So, the automation of your Machine Learning algorithms which will reduce the daily task of Data Scientists. This gives us the leverage to skip the selection of baseline models. By using this, we can simply get to an understanding of which model to go with and then by finalizing the model, we can carry out out optimization on it to get better\u00a0results.</p>\n<p>AutoML helps in automatic training and tuning of many models within a user-specified time limit. Some of the key features of H2OAutoML are it can do data pre-processing(encoding), data cleaning(missing value imputation) also provides us nice leaderboard view of the models with various metrics. So, we can pick any model from it and analyze the model output further. It also provides a deployment ready code. It gives in multiple formats like Mojo, Pojo, Binary formats. Out of which Mojo is the recommended format when the model size is huge. It uses GPU\u2019s for XGBoost\u00a0model.</p>\n<h3>Architecture:</h3>\n<p>H2O architecture can be divided into different layers in which the top layer will be different APIs, and the bottom layer will be H2O\u00a0JVM.</p>\n<p>H2O\u2019s core code is written in Java that enables the whole framework for multi-threading. Even though it is written in Java, it provides interfaces for R, Python and few others shown in the architecture.</p>\n<p>In short, we can say that H2O is an open source, in memory, distributed, fast and scalable machine learning and predictive analytics that allow building machine learning models to be an\u00a0ease.</p>\n<p>Now let me walk you through some use case which will give you some clarity on how it\u00a0works.</p>\n<p>For this demonstration, I am taking a imbalanced dataset which is Bank Churn dataset from Kaggle. You can find that\u00a0<a href=\"https://www.kaggle.com/adammaus/predicting-churn-for-bank-customers\">here</a>.</p>\n<p>Lets go through the\u00a0code.</p>\n<p><strong><em>Step-1</em></strong>: To install H2O, you need to have java run time environment as it is developed on java. So, install the java run time environment.</p>\n<pre>!apt-get install default-jre<br>!java -version</pre>\n<p><strong><em>Step-2</em></strong>: Install H2O and import\u00a0it.</p>\n<pre>!pip install h2o<br>import h2o</pre>\n<p><strong><em>Step-3</em></strong>: Initialize the H2O\u00a0cluster.</p>\n<pre>h2o.init()</pre>\n<p>The h2o.init() command is pretty smart and does a lot of work. At first, it looks for any active h2o instance before starting a new one and then starts a new one when instance are not present. Once the instance is initiated, we can see the <strong><em>flow</em></strong> running on <a href=\"http://127.0.0.1:54321/\">http://127.0.0.1:54321</a>.</p>\n<p><strong><em>Step-4</em></strong>: Load the data to H2O\u00a0frame.</p>\n<pre>from h2o.automl import H2OAutoML<br>bank_data = h2o.import_file('Churn.csv')</pre>\n<p>The reason why I have chosen this dataset is it has class imbalance and some decent data preprocessing needs to be done and see how AutoML deals with all\u00a0this.</p>\n<p><strong><em>Step-5</em></strong>: EDA of the\u00a0data.</p>\n<pre>bank_data.describe()<br>bank_data.types</pre>\n<p>Can do EDA\u00a0, but this is not our main focus, so skipping this. The dataset is having 14 columns out of which the target variable is \u201cExited\u201d(which says whether a customer has churned or not) there are some features like \u201cRowNumber\u201d, \u201cCustomerID\u201d, \u201cSurname\u201d which can be removed. There are some features which are categorical(Gender, Geography).</p>\n<p><strong><em>Step-6</em></strong>: Split the data to train, test sets with a split of 80% and 20% respectively.</p>\n<pre>train, test= bank_data.split_frame(ratios = [.8], seed = 1234)</pre>\n<p><strong><em>Step-7</em></strong>: selecting the predictors and the predicted variable.</p>\n<pre>y = 'Exited'<br>x = bank_data.columns<br>x.remove(y)<br>x.remove(\"RowNumber\")<br>x.remove(\"CustomerId\")<br>x.remove(\"Surname\")</pre>\n<p><strong><em>Step-8</em></strong>: Usage of H2OAutoML.</p>\n<pre>aml = H2OAutoML(max_models=20, seed = 10, balance_classes=True, exclude_algos = [\"StackedEnsemble\", \"DeepLearning\"], verbosity = \"info\", nfolds=0)</pre>\n<p>So, here I want the top 20 models, and we have a class imbalance so set it to True would balance the data and I don\u2019t want the algorithms like Stacked Ensemble, Deep Learning. So I can exclude them. And cross validations as 0 to keep it simple, But you can tweak this for better results. By default \u201cnfolds\u201d will be set to 5. Also, we can use some time boxes such that the models will not run for more than that specified time.</p>\n<p><strong><em>Step-9</em></strong>: Call the train function by passing input features and the output column and training data\u00a0frame.</p>\n<pre>aml.train(x=x,y=y,training_frame= train)</pre>\n<p>It will train for 20 different models and each time leader board gets\u00a0updated.</p>\n<p><strong><em>Step-10</em></strong>: Check the leaderboard to see the top performing models.</p>\n<pre>lb = aml.leaderboard<br>lb.head()</pre>\n<p>In this case, GBM tops the list then comes\u00a0XGBoost.</p>\n<p><strong><em>Step-11</em></strong>: Use the \u201cleader\u201d and predict on test\u00a0dataset.</p>\n<pre>prediction = aml.leader.predict(test)</pre>\n<p>And you can see the predictions with probabilities being churned or\u00a0not.</p>\n<p><strong><em>Step-12</em></strong>: Generate a performance report.</p>\n<pre>aml.leader.model_performance(test)</pre>\n<p>This report shows the metrics of the\u00a0model.</p>\n<p><strong><em>Step-13</em></strong>: So out of all the models, lets take a particular model from the leaderboard and analyze it further. Here I want XGBoost to be analyzed further. Lets take this XGBoost\u00a0model.</p>\n<pre>model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])<br>xgb=h2o.get_model([mid for mid in model_ids if \"XGBoost\" in mid][0])</pre>\n<p><strong><em>Step-14</em></strong>: Let see the output which gives us the model details and the\u00a0metrics.</p>\n<pre>xgb</pre>\n<p><strong><em>Step-15</em></strong>: Plot for the variable importance which gives us the list of the most significant variables. The top variables contribute more to the model than the bottom ones and also have high predictive power in classifying churn and no-churn customers.</p>\n<pre>xgb.varimp_plot()</pre>\n<h3>Conclusion:</h3>\n<p>H2O provides an easy-to-use open source platform for applying different ML algorithms on a given dataset. During testing, you can fine tune the parameters to these algorithms. H2O supports AutoML that provides the ranking amongst the several algorithms based on their performance. It can also handle Big Data. This is definitely a boon for Data Scientist to apply the different Machine Learning models on their dataset and pick up the best one to meet their\u00a0needs.</p>\n<p>Happy Learning\u2026\u2026\u2026\u2026</p>\n<p>Thanks for reading the article! If you like my article do \ud83d\udc4f this article. If you want to connect with me in Linkedin, please click\u00a0<a href=\"http://www.linkedin.com/in/karteek-menda\">here</a>.</p>\n<p>I will try to explain <strong>H2O Flow</strong> in my upcoming articles.</p>\n<p>This is Karteek\u00a0Menda.</p>\n<p>Signing Off</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a88806206782\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["machine-learning","automl","xgboost"]}]}